	var aDataSet = [
		/* auto-generated - START */
		['1','A Conversational Question Answering Challenge (CoQA)','English','Dataset for measuring the ability of machines to understand a text passage and answer a series of interconnected questions that appear in a conversation.','127,000+','JSON','Question Answering, Reading Comprehension','2019','Redy et al.','https://stanfordnlp.github.io/coqa/','https://arxiv.org/pdf/1808.07042.pdf',],
		['2','A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning (CLEVR &amp; CoGenT)','English','Visual question answering dataset contains 100,000 images and 999,968 questions.','999,968 questions; 100,000 images','JSON','Question Answering, Visual','2016','Johnson et al.','https://cs.stanford.edu/people/jcjohns/clevr/','http://vision.stanford.edu/pdf/johnson2017cvpr.pdf',],
		['3','A Novel Approach to a Semantically-Aware Representation of Items (NASARI)','Multi-Lingual','Dataset contains semantic vector representations for BabelNet synsets and Wikipedia pages in several languages: English, Spanish, French, German and Italian. Currently available three vector types: lexical, unified and embedded.','610K-4.4M depending on language','Text','Semantic Similarity','2016','Camacho-Collados et al.','http://lcl.uniroma1.it/nasari/','https://www.aclweb.org/anthology/N15-1059.pdf',],
		['4','A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs (DROP)','English','Dataset is used to resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).','96,000','JSON','Question Answering, Reading Comprehension','2019','Dua et al.','https://allennlp.org/drop','https://arxiv.org/pdf/1903.00161.pdf',],
		['5','ABC Australia News Corpus','English','Entire news corpus of ABC Australia from 2003 to 2017.','1,103,664','CSV','Clustering, Events, Sentiment Analysis','2017','Kulkarni','https://www.kaggle.com/therohk/million-headlines','',],
		['6','Abductive Natural Language Inference (aNLI)','English','Dataset is a binary-classification task, the goal is to pick the most plausible explanatory hypothesis given two observations from narrative contexts. It contains 20k commonsense narrative contexts and 200k explanations.&#34;','20,000','JSON','Classification, Commonsense','2019','Bhagavatula et al.','https://leaderboard.allenai.org/anli/submissions/get-started','https://arxiv.org/pdf/1908.05739.pdf',],
		['7','Academic','English','Questions about the Microsoft Academic Search (MAS) database, derived by enumerating every logical query that could be expressed using the search page of the MAS website and writing sentences to match them.','196','JSON','Semantic Parsing, Text-to-SQL','2014','Li et al.','https://github.com/jkkummerfeld/text2sql-data','http://www.vldb.org/pvldb/vol8/p73-li.pdf',],
		['8','Activitynet-QA','English','Dataset contains 58,000 human-annotated QA pairs on 5,800 videos derived from the popular ActivityNet dataset. The dataset provides a benckmark for testing the performance of VideoQA models on long-term spatio-temporal.','58,000','JSON','Question Answering, Visual, Commonsense','2019','Yu et al.','https://github.com/MILVLG/activitynet-qa','https://arxiv.org/pdf/1906.02467.pdf',],
		['9','Advising','English','Dataset contains questions regarding course information at the University of Michigan, but with fictional student records.','4,570','JSON','Semantic Parsing, Text-to-SQL','2018','Finegan-Dollak et al.','https://github.com/jkkummerfeld/text2sql-data','https://arxiv.org/pdf/1806.09029.pdf',],
		['10','Affective Text','English','Classification of emotions in 250 news headlines. Categories: anger, disgust, fear, joy, happiness, sadness, surprise.','250','SGML, Text','Emotion Classification','2007','Strapparava et al.','https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets','https://web.eecs.umich.edu/~mihalcea/papers/strapparava.semeval07.pdf',],
		['11','AG News','English','Dataset contains more than 1 million news articles for topic classification. The 4 classes are: World, Sports, Business, and Sci/Tech.','1M+','CSV','Classification','2015','Zhang et al.','https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz','http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html',],
		['12','AI2 Reasoning Challenge (ARC)','English','Dataset contains 7,787 genuine grade-school level, multiple-choice science questions.','7,787','JSON, CSV','Question Answering, Reading Comprehension','2018','Clark et al.','http://data.allenai.org/arc/','https://arxiv.org/pdf/1803.05457.pdf',],
		['13','AI2 Science Questions Mercury','English','Dataset consists of questions used in student assessments across elementary and middle school grade levels. Includes questions with diagrams and without.','6,940','JSON, JPG','Reading Comprehension','2017','Allen Institute','http://data.allenai.org/ai2-science-questions-mercury/','',],
		['14','AI2 Science Questions v2.1','English','Dataset consists of questions used in student assessments in the United States across elementary and middle school grade levels. Each question is 4-way multiple choice format and may or may not include a diagram element.','5,060','JSON, CSV','Question Answering, Reading Comprehension','2017','Allen Institute','http://data.allenai.org/ai2-science-questions/','',],
		['15','Amazon Fine Food Reviews','English','Dataset consists of reviews of fine foods from amazon.','568,454','CSV','Classification, Sentiment Analysis','2013','McAuley et al.','https://www.kaggle.com/snap/amazon-fine-food-reviews','http://i.stanford.edu/~julian/pdfs/www13.pdf',],
		['16','Amazon Reviews','English','US product reviews from Amazon.','233.1M','JSON','Classification, Sentiment Analysis','2018','McAuley et al.','https://nijianmo.github.io/amazon/index.html','http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf',],
		['17','An Open Information Extraction Corpus (OPIEC)','English','OPIEC is an Open Information Extraction (OIE) corpus, constructed from the entire English Wikipedia containing more than 341M triples.','341M','AVRO','Knowledge Base, Information Extraction, Knowledge Base','2019','Gashteovski et al.','https://www.uni-mannheim.de/en/dws/research/resources/opiec/','https://arxiv.org/pdf/1904.12324.pdf',],
		['18','AQuA','English','Dataset containing algebraic word problems with rationales for their answers.','100,000','JSON','Question Answering, Reading Comprehension','2017','Ling et al.','https://github.com/deepmind/AQuA','https://arxiv.org/pdf/1705.04146.pdf',],
		['19','Arabic Jordanian General Tweets (AJGT)','Arabic','Dataset consists of 1,800 tweets annotated as positive and negative. Modern Standard Arabic (MSA) or Jordanian dialect.','1,800','Excel','Classification, Sentiment Analysis','2017','Alomari','https://github.com/komari6/Arabic-twitter-corpus-AJGT','',],
		['20','Arabic Reading Comprehension Dataset (ARCD)','Arabic','Dataset contains 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD) containing 48,344 questions.','~50,000','JSON','Question Answering, Reading Comprehension','2019','Mozannar et al.','https://github.com/husseinmozannar/SOQAL','https://arxiv.org/pdf/1906.05394.pdf',],
		['21','Arabic Speech Corpus','Arabic','Dataset was recorded in south Levantine Arabic (Damascian accent) using a professional studio. Synthesized speech as an output using this corpus has produced a high quality, natural voice.','n/a','WAV, LAB','Speech Corpora','2016','Halabi','http://en.arabicspeechcorpus.com/','http://en.arabicspeechcorpus.com/Nawar%20Halabi%20PhD%20Thesis%20Revised.pdf',],
		['22','Arabic Violence Twitter Corpus','Arabic','Annotated Arabic tweets which mention a violent act. Tweets were classifed into 8 classes: Crime, Accident, Crisis, Conflict, Human Rights Abuse, Violence, Opinion, or other. Requires using Twitter API to match IDs with tweets for retrieval.','20,000','Text','Classification','2016','Ayman et al.','https://github.com/Alhelbawy/Arabic-Violence-Twitter','https://www.aclweb.org/anthology/L16-1257.pdf',],
		['23','ArabicWeb16','Arabic','Dataset contains 150,211,934 Arabic Web pages with high coverage of dialectal Arabic as well as Modern Standard Arabic (MSA).','150M','WARC','Text Corpora','2016','Suwaileh et al.','https://sites.google.com/view/arabicweb16/download/arabicweb16?authuser=0','https://www.ischool.utexas.edu/~ml/papers/sigir16-arabicweb.pdf',],
		['24','Argentinian Spanish [es-ar] Speech Multi-Speaker Dataset.','Spanish (Argentinan)','Speech dataset containing about 5,900 transcribed high-quality audio from Argentinian Spanish [es-ar] sentences recorded by volunteers.','~5,900','Wav','Speech Recognition','2018','Google','http://openslr.org/61/','',],
		['25','ArguAna TripAdvisor Corpus','English','Dataset contains 2,100 hotel reviews balanced with respect to the reviews? sentiment scores. reviews are segmented into subsentence-level statements that have been manually classified as a fact, a positive, or a negative opinion.','2,100','XMI','Classification, Sentiment Analysis','2014','Wachsmuth et al.','http://argumentation.bplaced.net/arguana/data','http://argumentation.bplaced.net/arguana-publications/papers/wachsmuth14a-cicling.pdf',],
		['26','Aristo Tuple KB','English','Dataset contains a collection of high-precision, domain-targeted (subject,relation,object) tuples extracted from text using a high-precision extraction pipeline, and guided by domain vocabulary constraints.','282,594','TSV','Knowledge Base','2017','Dalvi et al.','http://data.allenai.org/tuple-kb/','http://ai2-website.s3.amazonaws.com/publications/comprehensive_knowledge_extraction-final.pdf',],
		['27','arXiv Bulk Data','English','A collection of research papers on arXiv.','n/a','Tar','Text Corpora','2011','n/a','https://arxiv.org/help/bulk_data_s3','https://www.researchgate.net/publication/332799542_On_the_Use_of_ArXiv_as_a_Dataset',],
		['28','ASTD: Arabic Sentiment Tweets Dataset','Arabic','Dataset contains over 10k Arabic sentiment tweets classified into 4 classes: subjective positive, subjective negative, subjective mixed, and objective.','10,000+','Text','Classification, Sentiment Analysis','2015','Nabil et al.','https://github.com/mahmoudnabil/ASTD','https://www.aclweb.org/anthology/D15-1299.pdf',],
		['29','ASU Twitter Dataset','English','Twitter network data, not actual tweets. Shows connections between a large number of users.','11,316,811 users, 85,331,846 connections','CSV','Clustering, Graph Analysis','2009','Zafarani et al.','http://socialcomputing.asu.edu/datasets/Twitter','',],
		['30','ATIS','English','Dataset is a collection of utterances to a flight booking system, accompanied by a relational database and SQL queries to answer the questions.','877','JSON','Semantic Parsing, Text-to-SQL','2017','Dahl/Iyer et al.','https://github.com/jkkummerfeld/text2sql-data','https://www.aclweb.org/anthology/P18-1033.pdf',],
		['31','AudioSet','Multi-Lingual','Dataset consists of an expanding ontology of 632 audio event classes and a collection of 2,084,320 human-labeled 10-second sound clips drawn from YouTube videos.','n/a','CSV, TFR','Speech Recognition, Visual','2017','Google','https://research.google.com/audioset/download.html','https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45857.pdf',],
		['32','Automated Essay Scoring','English','Dataset contains student-written essays with scores.','n/a','TSV, xlsx','Scoring Classification','2017','The Hewlett Foundation','https://www.kaggle.com/c/asap-aes/overview','https://www.researchgate.net/publication/26415982_Automated_Essay_Scoring',],
		['33','Automatic Keyphrase Extraction','English','Multiple datasets for automatic keyphrase extraction.','n/a','Multiple','Information Retrieval','1999-2008','Several','https://github.com/snkim/AutomaticKeyphraseExtraction','https://www.aclweb.org/anthology/P14-1119.pdf',],
		['34','bAbI 20 Tasks','English, Hindi','Dataset cotains a set of contexts, with multiple question-answer pairs available based on the contexts.','2,000','Text','Question Answering, Reading Comprehension','2015','Weston et al.','https://research.fb.com/downloads/babi/','https://arxiv.org/pdf/1502.05698.pdf',],
		['35','babI 6 Tasks Dialogue','English','Dataset contains 6 tasks for testing end-to-end dialog systems in the restaurant domain.','3,000','Text','Dialogue','2017','Bordes et al.','https://research.fb.com/downloads/babi/','https://arxiv.org/pdf/1605.07683.pdf',],
		['36','Background Knowledge Dialogue Dataset','English','Dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie.','90,000','JSON','Dialogue','2018','Moghe et al.','https://github.com/nikitacs16/Holl-E','https://arxiv.org/pdf/1809.08205.pdf',],
		['37','BBC','English','Two news article datasets, originating from BBC News, provided for use as benchmarks for machine learning research. ','','','','','','http://mlg.ucd.ie/datasets/bbc.html','',],
		['38','Ben Gurion University Social Networks Security Research Group','English','Anonymized versions of directed, undirected, and multi-label social networks datasets','','','','','','http://proj.ise.bgu.ac.il/sns/datasets.html ','',],
		['39','Bianet','Multi-Lingual',' Dataset is a parallel news corpus with 3,214 Turkish articles with their sentence-aligned Kurdish or English translations from the Bianet online newspaper. Requires a request submission for dataset.','3,214','XML','Machine Translation','2018','Ataman et al.','https://d-ataman.github.io/bianet/','http://lrec-conf.org/workshops/lrec2018/W19/pdf/6_W19.pdf',],
		['40','Bible Corpus','Multi-Lingual','A parallel corpus created from translations of the Bible containing 102 languages.','2.84M','XML','Machine Translation','2014','Christodoulopoulos et al.','http://opus.nlpl.eu/bible-uedin.php','https://www.researchgate.net/publication/269334313_A_massively_parallel_corpus_the_Bible_in_100_languages',],
		['41','Blacklist Keywords','English, Chinese','Keyword blacklists and lists of other content such as URLs or images used to trigger censorship in apps used in China. With the exception of WeChat, these lists were reverse engineered and are the exhaustive lists of keywords used to trigger censorship on these platforms. ','','','','','','https://github.com/citizenlab/chat-censorship/tree/81e91a6e99e904da2a275a959bc9aff437c9347d ','',],
		['42','BlogFeedback Dataset','English','Dataset to predict the number of comments a post will receive based on features of that post.','60,021','Text','Regression','2014','Buza','https://archive.ics.uci.edu/ml/machine-learning-databases/00304/','http://www.cs.bme.hu/~buza/pdfs/gfkl2012_blogs.pdf',],
		['43','Blogger Authorship Corpus','English','Blog post entries of 19,320 people from blogger.com.','681,288','Text','Classification, Sentiment Analysis','2006','Schler et al.','https://u.cs.biu.ac.il/~koppel/BlogCorpus.htm','http://u.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf',],
		['44','Blogger Corpus','English','Collected posts of 19,320 bloggers gathered from blogger.com.','','','','','','http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip','',],
		['45','Books Corpus','Multi-Lingual','Dataset contains a collection of copyright free books. Corpus consists of 16 languages and 0.91M sentence fragments and 19.50M tokens.','0.91M','XCES, XML','Machine Translation','2012','Tiedemann','http://opus.nlpl.eu/Books.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['46','BoolQ','English','Question answering dataset for yes/no questions.','15,942','JSON','Binary Question Answering','2019','Clark et al.','https://github.com/google-research-datasets/boolean-questions','https://arxiv.org/pdf/1905.10044.pdf',],
		['47','Break','English','Dataset contains 83,978 examples sampled from 10 question answering datasets over text, images and databases. Dataset used to obtain the Question Decomposition Meaning Representation (QDMR) for questions.','83,978','CSV','Natural Question Understanding (NQU)','2020','Wolfson et al.','https://allenai.github.io/Break/','https://arxiv.org/pdf/2001.11770v1.pdf',],
		['48','BSNLP-2019','Multi-Lingual','Dataset used to classify named entities in web documents in Slavic languages, their lemmatization, and cross-language matching. Dataset covers 4 languages: Bulgarian, Czech, Polish, and Russian.','n/a','Text, OUT','Named Entiry Recognition (NER), Entity Linking','2019','Piskorski et al.','http://bsnlp.cs.helsinki.fi/shared_task.html','http://bsnlp.cs.helsinki.fi/shared_task_BNSLP_2019.pdf',],
		['49','Buzz in Social Media Dataset','English','Data from Twitter and Tom&#39;s Hardware. This dataset focuses on specific buzz topics being discussed on those sites.','140,000','Text','Classification','2013','Kawala et al.','https://archive.ics.uci.edu/ml/machine-learning-databases/00248/','https://hal.archives-ouvertes.fr/hal-00881395/document',],
		['50','CAPES','English, Portuguese','A parallel corpus of theses and dissertation abstracts in Portuguese and English from CAPES.','2.32M','XML','Machine Translation','2012','Tiedemann et al.','http://opus.nlpl.eu/CAPES.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['51','Car Evaluation Dataset','English','Car properties and their overall acceptability.','1,728','Text','Classification','1997','Bohanec','https://archive.ics.uci.edu/ml/machine-learning-databases/car/','https://pdfs.semanticscholar.org/df46/10614ccb66a8cd8914d0ea6d191e100e1bd2.pdf?_ga=2.50792299.354477249.1582324561-958501894.1582324561',],
		['52','CASS','French','Dataset is composed of decisions made by the French Court of cassation and summaries of these decisions made by lawyer.','129,445','XML','Summarization','2019','Bouscarrat et al.','https://github.com/euranova/CASS-dataset','https://www.data.gouv.fr/fr/datasets/cass/',],
		['53','CCMatrix','Multi-Lingual','4.5 billion parallel sentences in 576 language pairs pulled from snapshots of the CommonCrawl public dataset.','4.5B','to be added soon','Machine Translation','2019','Schwenk et al.','https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix','https://arxiv.org/pdf/1911.04944.pdf',],
		['54','Children?s Book Test (CBT)','English','Dataset contains ?questions? from chapters in the book by enumerating 21 consecutive sentences. In each question, the first 20 sentences form the context, and a word is removed from the 21st sentence, which becomes the query. Models must identify the answer word among a selection of 10 candidate answers appearing in the context sentences and the query.','~688,000','Text','Question Answering, Reading Comprehension','2016','Hill et al.','https://research.fb.com/downloads/babi/','https://research.fb.com/wp-content/uploads/2016/11/the_goldilocks_principle_reading_children_s_books_with_explicit_memory_representations.pdf?',],
		['55','Chinese Machine Reading Comprehension (CMRC 2018)','Chinese','Dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts.','20,000','JSON','Question Answering, Reading Comprehension','2018','Cui et al.','https://github.com/ymcui/cmrc2018','https://arxiv.org/pdf/1810.07366.pdf',],
		['56','Chinese Word Vectors','English','This project provides 100+ Chinese Word Vectors (embeddings) trained with different representations (dense and sparse), context features (word, ngram, character, and more), and corpora. One can easily obtain pre-trained vectors with different properties and use them for downstream tasks.','','','','','','https://github.com/Embedding/Chinese-Word-Vectors','',],
		['57','Choice of Plausible Alternatives (COPA)','English','Dataset used for open-domain commonsense causal reasoning.','1,000','XML','Commonsense Reasoning','2011','Roemmele et al.','http://people.ict.usc.edu/~gordon/copa.html','http://commonsensereasoning.org/2011/papers/Roemmele.pdf',],
		['58','Classify Emotional Relationships of Fictional Characters','English','Dataset contains 19 short stories that are shorter than 1,500 words, and depict at least four different characters.','19','Text','Text Corpora, Emotion Classification','2019','Kim et al.','https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/relationalemotions/','https://arxiv.org/pdf/1903.12453.pdf',],
		['59','Clinical Case Reports for Machine Reading Comprehension (CliCR)','English','Dataset was built from clinical case reports, requiring the reader to answer the query with a medical problem/test/treatment entity.','100,000','JSON','Question Answering, Reading Comprehension','2018','?uster et al.','https://github.com/clips/clicr','https://arxiv.org/pdf/1803.09720.pdf',],
		['60','ClueWeb Corpora','English','Annotated web pages from the ClueWeb09 and ClueWeb12 corpora.','340,451,982','Text','Classification','2013','Gabrilovich et al.','http://lemurproject.org/clueweb09/FACC1/','http://lemurproject.org/clueweb09/',],
		['61','CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI)','English','Dataset contains more than 23,500 sentence utterance videos from more than 1000 online YouTube speakers. The dataset is gender balanced. All the sentences utterance are randomly chosen from various topics and monologue videos.','23,500','n/a','Sentiment Analysis, Emotion Recognition, Visual','2018','MultiComp Lab','https://github.com/A2Zadeh/CMU-MultimodalSDK','https://www.aclweb.org/anthology/P18-1208.pdf',],
		['62','CMU Social-IQ Dataset','English',' Social-IQ dataset contains a total of 7500 questions (6 per video). Figure 2 (a) demonstrates the distribution of question length in terms of number of words. The average length of questions in Social-IQ is 10.87 words. Social-IQ contains a total of 30,000 correct (4 per question) and 22,500 (3 per question) incorrect answers. Social-IQ dataset consists of a total of 1,250 videos from YouTube. Figure 2 (f) demonstrates an overview of categories of the videos in Social-IQ. There is a total of 1,239 minutes of annotated video content (across 10,529 minutes of full videos).','','','','','','https://github.com/A2Zadeh/Social-IQ, https://github.com/A2Zadeh/CMU-MultimodalSDK, immortal.multicomp.cs.cmu.edu/raw_datasets/Social-IQ.zip','',],
		['63','CNN / Daily Mail Dataset','English','Cloze-style reading comprehension dataset created from CNN and Daily Mail news articles.','1M+','Question','Question Answering, Reading Comprehension','2015','Hermann et al.','https://cs.nyu.edu/~kcho/DMQA/','https://arxiv.org/pdf/1506.03340.pdf',],
		['64','Coached Conversational Preference Elicitation','English','Dataset consisting of 502 English dialogs with 12,000 annotated utterances between a user and an assistant discussing movie preferences in natural language.','12,000','JSON','Dialogue','2019','Radlinski et al.','https://research.google/tools/datasets/coached-conversational-preference-elicitation/','https://www.aclweb.org/anthology/W19-5941.pdf',],
		['65','Coarse Discourse','English','Dataset contains discourse annotations and relations on threads from Reddit during 2016. Requires merging using Reddit API.','9,473','JSON','Text Corpora','2017','Zhang et al.','https://github.com/google-research-datasets/coarse-discourse','https://storage.googleapis.com/pub-tools-public-publication-data/pdf/60e5f386a058bb5b0b706dc5ff34746ee7ec8fab.pdf',],
		['66','Code-Mixed-Dialog','Multi-Lingual','A goal-oriented dialog dataset containing code-mixed conversations. Specifically, text from the DSTC2 restaurant reservation dataset and create code-mixed versions of it in Hindi-English, Bengali-English, Gujarati-English and Tamil-English.','49,167','Text','Dialogue','2018','Banerjee et al.','https://github.com/sumanbanerjee1/Code-Mixed-Dialog','https://arxiv.org/pdf/1806.05997.pdf',],
		['67','CommitmentBank','English','Dataset contains naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator (question, modal, negation, antecedent of conditional).','1,200','CSV','Entailment, Inference','2019','Marneffe et al.','https://github.com/mcdm/CommitmentBank','https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf',],
		['68','Common Objects in Context (COCO)','English','COCO is a large-scale object detection, segmentation, and captioning dataset. Dataset contains 330K images (&gt;200K labeled) 1.5 million object instances, 80 object categories, 91 stuff categories, 5 captions per image.','330,000','JSON, JPG','Automatic Image Captioning','2014','Lin et al.','http://cocodataset.org/','https://arxiv.org/pdf/1405.0312.pdf',],
		['69','Common Voice','Multi-Lingual','Dataset containing audio in 29 languages and 2,454 recorded hours .','n/a','MP3','Speech Recognition','2019','Mozilla','https://voice.mozilla.org/en/datasets','https://arxiv.org/pdf/1912.06670.pdf',],
		['70','CommonCrawl','Multi-Lingual','Dataset contains data from 25 billion web pages. Common Crawl, a non-profit organization, provides an open repository of web crawl data that is freely accessible to all. In doing so, we aim to advance the open web and democratize access to information','25B','WET','Text Corpora','2013-2019','Common Crawl Foundation','http://commoncrawl.org/the-data/get-started/','https://docs.google.com/file/d/1_9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb_uL5N9/edit',],
		['71','CommonGen','English','Dataset consists of 30k concept-sets with humanwritten sentences as references.','30,000','JSON','Text Generation','2019','Lin et al.','http://inklab.usc.edu/CommonGen/','http://inklab.usc.edu/CommonGen/commongen_acl20.pdf',],
		['72','COmmonsense Dataset Adversarially-authored by Humans (CODAH)','English','Commonsense QA in the sentence completion style of SWAG. As opposed to other automatically generated NLI datasets, CODAH is adversarially constructed by humans who can view feedback from a pre-trained model and use this information to design challenging commonsense questions.','2,776','TSV','Question Answering, Reading Comprehension, Commonsense','2019','Chen et al.','https://github.com/Websail-NU/CODAH','https://arxiv.org/pdf/1904.04365.pdf',],
		['73','CommonsenseQA','English','Dataset contains multiple-choice question answering dataset that requires different types of commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct answer and four distractor answers.','12,012','JSON','Question Answering, Reading Comprehension, Commonsense','2018','Talmor et al.','https://www.tau-nlp.org/commonsenseqa','https://arxiv.org/pdf/1811.00937.pdf',],
		['74','Complex Factoid Question Answering with Paraphrase Clusters (ComQA)','English','The dataset contains questions with various challenging phenomena such as the need for temporal reasoning, comparison (e.g., comparatives, superlatives, ordinals), compositionality (multiple, possibly nested, subquestions with multiple entities), and unanswerable questions.','11,214','JSON','Question Answering, Reading Comprehension','2019','Abujabal et al.','http://qa.mpi-inf.mpg.de/comqa/','https://www.aclweb.org/anthology/N19-1027.pdf',],
		['75','ComplexWebQuestions','English','Dataset contains a large set of complex questions in natural language, and can be used in multiple ways.','34,689','JSON','Question Answering, Reading Comprehension','2018','Talmor et al.','https://www.tau-nlp.org/compwebq','https://arxiv.org/pdf/1803.06643.pdf',],
		['76','Conceptual Captions','English','Dataset contains ~3.3M images annotated with captions to be used for the task of automatically producing a natural-language description for an image.','3,318,333','TSV','Automatic Image Captioning','2018','Sharma et al.','https://ai.google.com/research/ConceptualCaptions/download','https://www.aclweb.org/anthology/P18-1238.pdf',],
		['77','Conference on Computational Natural Language Learning (CoNLL 2002)','Spanish, Dutch','Spanish data is a collection of newswire articles made available by the Spanish EFE News Agency.The Dutch data consist of four editions of the Belgian newspaper &#34;De Morgen&#34; of 2000. IOB2 format.','n/a','HTML','Named Entity Recognition (NER)','2002','Tjong et al.','https://www.clips.uantwerpen.be/conll2002/ner/','https://www.aclweb.org/anthology/W02-2024.pdf',],
		['78','Conference on Computational Natural Language Learning (CoNLL 2003)','English, German','Dataset contains news articles whose text are segmented in 4 columns: the first item is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag.','English 1,393; German 909','Tar','Text Corpora, Named Entity Recognition (NER), Part-of-Speech (POS)','2003','Sang et al.','https://www.clips.uantwerpen.be/conll2003/ner/','https://www.aclweb.org/anthology/W03-0419.pdf',],
		['79','Content-Based Categorized Dataset','Arabic','Dataset contains 996 Web pages from the ArabicWeb16 dataset were extracted and labeled.','996','Text','Text Classification','2016','Suwaileh et al.','https://sites.google.com/view/arabicweb16/download/labelled-datasets?authuser=0','https://www.ischool.utexas.edu/~ml/papers/sigir16-arabicweb.pdf',],
		['80','Conversational Text-to-SQL Systems (CoSQL)','English','Dataset consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz collection of 3k dialogues querying 200 complex databases spanning 138 domains.It is the dilaogue version of the Spider and SParC tasks.','3,000','JSON, SQL','Dialogue, SQL-to-Text','2019','Yu et al.','https://yale-lily.github.io/cosql','https://arxiv.org/pdf/1909.05378.pdf',],
		['81','Cornell Movie--Dialogs Corpus','English','This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts. 220,579 conversational exchanges between 10,292 pairs of movie characters, involves 9,035 characters from 617 moviesin. total 304,713 utterances.','304,713','Text','Dialogue','2011','Danescu et al.','http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html','https://arxiv.org/pdf/1106.3077.pdf',],
		['82','Cornell Natural Language for Visual Reasoning (NLVR and NLVR2)','English','Dataset contains two language grounding datasets containing natural language sentences grounded in images. The task is to determine whether a sentence is true about a visual input.','NLVR2 107,292; NLVR 92,244','JSON','Question Answering, Visual','2019','Suhr et al.','http://lil.nlp.cornell.edu/nlvr/','https://arxiv.org/pdf/1811.00491.pdf',],
		['83','Cornell Newsroom','English','Dataset contains 1.3 million articles and summaries written by authors and editors in the newsrooms of 38 major publications. The summaries are obtained from search and social metadata between 1998 and 2017.','1.3M','JSON','Text Corpora, Summarization','2018','Grusky et al.','https://summari.es/download/','https://www.aclweb.org/anthology/N18-1065.pdf',],
		['84','Corporate Messaging Corpus','English','Dataset contains classifed statements as information, dialog (replies to users, etc.), or action (messages that ask for votes or ask users to click on links, etc.','3,118','CSV','Classification','2015','Crowdflower','https://data.world/crowdflower/corporate-messaging','',],
		['85','Cosmos QA','English','Dataset containing thousands of problems that require commonsense-based reading comprehension, formulated as multiple-choice questions.','35,000','CSV','Question Answering, Reading Comprehension, Commonsense','2019','Huang et al.','https://github.com/wilburOne/cosmosqa/tree/master/data/','https://arxiv.org/pdf/1909.00277.pdf',],
		['86','COVID-19 Open Research Dataset (CORD-19)','English','Dataset contains 44,000 scholarly articles, including over 29,000 with full text, about COVID-19 and the coronavirus family of viruses for use by the global research community.','44,000','JSON','Text Corpora','2020','Allen Institute','https://pages.semanticscholar.org/coronavirus-research','',],
		['87','Curation Corpus','English','Dataset is a collection of 40,000 professionally-written summaries of news articles, with links to the articles themselves.','40,000','CSV','Text Corpora','2020','Curation Corporation','https://github.com/CurationCorp/curation-corpus','',],
		['88','Customer Interaction Data of German Emails and Online Requests','German','Dataset is used to evaluate the task of automatically categorizing German customer requests. The dataset consists of a set emails and online requests sent to the support center of a multimedia software company.','627','XML','Text Corpora','2014','Eichler et al.','http://www.dfki.de/~neumann/resources/omqdata.html','http://www.dfki.de/~neumann/publications/new-ps/starsem2014-FINAL.pdf',],
		['89','DailyDialog','English','A manually labelled conversations dataset. Categories: no emotion, anger, disgust, fear, happiness, sadness, surprise.','13,118','Text','Emotion Classification','2017','Li et al.','http://yanran.li/dailydialog.html','https://www.aclweb.org/anthology/I17-1099.pdf',],
		['90','Danish-Similarity-Dataset','Danish','Dataset consists of 99 word pairs rated by 38 human judges according to their semantic similarity.','99','CSV','Semantic Textual Similarity','2019','Schneidermann','https://comparable.limsi.fr/bucc2018/bucc2018-task.html','',],
		['91','Dataset for Fill-in-the-Blank Humor','English','Dataset contains 50 fill-in-the-blank stories similar in style to Mad Libs. The blanks in these stories include the original word and the hint type (e.g. animal, food, noun, adverb).','50','JSON','Text Generation','2017','Hossain et al.','https://www.microsoft.com/en-us/download/details.aspx?id=55593','https://www.aclweb.org/anthology/D17-1067.pdf',],
		['92','Dataset for Intent Classification and Out-of-Scope Prediction','English','Dataset is a benchmark for evaluating intent classification systems for dialog systems / chatbots in the presence of out-of-scope queries.','23,000+','JSON','Intent Classification','2019','Larson et al.','https://github.com/clinc/oos-eval/tree/master/data','https://arxiv.org/pdf/1909.02027.pdf',],
		['93','Dataset for the Machine Comprehension of Text','English','Stories and associated questions for testing comprehension of text.','660','Text','Question Answering, Reading Comprehension','2013','Richardson et al.','https://github.com/mcobzarenco/mctest/tree/master/data/MCTest','https://www.aclweb.org/anthology/D13-1020.pdf',],
		['94','Dbpedia','Multi-Lingual','The English version of the DBpedia knowledge base currently describes 6.6M entities of which 4.9M have abstracts, 1.9M have geo coordinates and 1.7M depictions. In total, 5.5M resources are classified in a consistent ontology.','6.6M','Multiple','Knowledge Base','2016','Dbpedia','https://wiki.dbpedia.org/develop/datasets/dbpedia-version-2016-10','http://svn.aksw.org/papers/2013/SWJ_DBpedia/public.pdf',],
		['95','Deal or No Deal? End-to-End Learning for Negotiation Dialogues','English','This dataset consists of 5,808 dialogues, based on 2,236 unique scenarios dealing with negotiations and complex communication.','5,808','Text','Dialogue','2017','Lewis et al.','https://github.com/facebookresearch/end-to-end-negotiator','https://arxiv.org/pdf/1706.05125.pdf',],
		['96','DeepMind News Articles for Q&A','English','Hermann et al. (2015) created two awesome datasets (CNN & Daily Mail) using news articles for Q&A research. Each dataset contains many documents (90k and 197k each), and each document companies on average 4 questions approximately. Each question is a sentence with one missing word/phrase which can be found from the accompanying document/context.','','','','','','http://cs.nyu.edu/~kcho/DMQA/','',],
		['97','Delta Reading Comprehension Dataset','Chinese','Dataset organizes 10,014 paragraphs from 2,108 wiki entries and highlights more than 30,000 questions from the paragraphs.','10,014','JSON','Question Answering, Reading Comprehension','2019','Shao et al.','https://github.com/DRCKnowledgeTeam/DRCD','https://arxiv.org/ftp/arxiv/papers/1806/1806.00920.pdf',],
		['98','Densely Annotated Wikipedia Texts (DAWT)','Multi-Lingual','Dataset contains a total of 13.6M articles across several languages: English, Spanish, Italian, German, French and Arabic. The annotations include labeled text mentions mapping to entities (represented by their Freebase machine ids) as well as the type of entity.','13.6M','JSON','Named Entity Recognition (NER)','2017','Spasojevic et al.','https://github.com/klout/opendata/tree/master/wiki_annotation','https://arxiv.org/pdf/1703.00948.pdf',],
		['99','DEXTER Dataset','English','Task given is to determine, from features given, which articles are about corporate acquisitions.','2,600','Text','Classification','2008','Reuters','https://archive.ics.uci.edu/ml/machine-learning-databases/dexter/','http://clopinet.com/isabelle/Projects/NIPS2003/Slides/NIPS2003-Datasets.pdf',],
		['100','Dialogue Natural Language Inference (NLI)','English','Dataset used to improve the consistency of a dialogue model. It consists of sentence pairs labeled as entailment (E), neutral (N), or contradiction (C).&#34;','340,000+','JSON','Dialogue, Entailment','2019','Welleck et al.','https://wellecks.github.io/dialogue_nli/','https://arxiv.org/pdf/1811.00671.pdf',],
		['101','DiscoFuse','English','Dataset contains examples for training sentence fusion models. Sentence fusion is the task of joining several independent sentences into a single coherent text. The data has been collected from Wikipedia and from Sports articles.','~60M','TSV','Sentence Fusion','2019','Geva et al.','https://github.com/google-research-datasets/discofuse','https://arxiv.org/pdf/1902.10526.pdf',],
		['102','DOGC','Catalan, Spanish','A collection of documents from the official journal of the Catalan Goverment in Catalan and Spanish.','21.87M','XML','Text Corpora, Machine Translation','2012','Tiedemann et al.','http://opus.nlpl.eu/DOGC.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['103','DSL Corpus Collection (DSLCC)','Multi-Lingual','Dataset contains short excerpts of journalistic texts in similar languages and dialects.','294,000','Text','Discriminating between similar languages','2017','Tang et al.','https://www.kaggle.com/vardial/dslcc','http://ttg.uni-saarland.de/resources/DSLCC/papers/bucc2014.pdf',],
		['104','DuReader','Mandarin','DuReader version 2.0 contains more than 300K question, 1.4M evidence documents and 660K human generated answers.','1,431,429','JSON','Question Answering, Reading Comprehension','2018','He et al.','http://ai.baidu.com/broad/download?dataset=dureader','https://arxiv.org/pdf/1711.05073.pdf',],
		['105','Dutch Book Reviews','Dutch','Dataset contains book reviews along with associated binary sentiment polarity labels.','118,516','Text','Classification, Sentiment Analysis','2019','van der Burgh','https://benjaminvdb.github.io/110kDBRD/','https://arxiv.org/pdf/1910.00896.pdf',],
		['106','DVQA','English','Dataset containing data visualizations and natural language questions.','3,487,194','JSON, PNG','Question Answering, Visual, Commonsense','2018','Kafle et al.','https://github.com/kushalkafle/DVQA_dataset','https://arxiv.org/pdf/1801.08163.pdf',],
		['107','ECB Corpus','Multi-Lingual','Website and documentation from the European Central Bank. Contains 19 languages.','30.55M','XML','Text Corpora, Machine Translation','2012','Tiedemann et al.','http://opus.nlpl.eu/ECB.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['108','EMEA','Multi-Lingual','A parallel corpus made out of PDF documents from the European Medicines Agency. Contains 22 languages.','26.51M','XML','Machine Translation','2012','Tiedemann et al.','http://opus.nlpl.eu/EMEA.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['109','EmoBank','English','Dataset is a large-scale text corpus manually annotated with emotion according to the psychological Valence-Arousal-Dominance scheme.','10,000','CSV','Text Corpora','2017','Buechel et al.','https://github.com/JULIELab/EmoBank','https://www.aclweb.org/anthology/E17-2092.pdf',],
		['110','Emotion-Stimulus','English','Dataset annotated with both the emotion and the stimulus using FrameNet?s emotions-directed frame. 820 sentences with both cause and emotion and 1594 sentences marked with their emotion tag. Categories: happiness, sadness, anger, fear, surprise, disgust and shame.','2,414','XML','Emotion Classification','2015','Ghazi et al.','http://www.site.uottawa.ca/~diana/resources/emotion_stimulus_data/','http://www.site.uottawa.ca/~diana/publications/90420152.pdf',],
		['111','EmpatheticDialogues','English','Dataset of 25k conversations grounded in emotional situations.','25,000','CSV','Dialogue','2019','Rashkin et al.','https://github.com/facebookresearch/EmpatheticDialogues','https://arxiv.org/pdf/1811.00207.pdf',],
		['112','Enron Email Dataset','English','Emails from employees at Enron organized into folders.','~500,000','Text','Text Corpora','2004 (2015)','Klimt et al.','https://www.cs.cmu.edu/~enron/','http://nyc.lti.cs.cmu.edu/yiming/Publications/klimt-ecml04.pdf',],
		['113','Eubookshop','Multi-Lingual','Corpus of documents from the EU bookshop. Contains 48 languages.','173.20M','XML','Text Corpora, Machine Translation','2012','Tiedemann et al.','http://opus.nlpl.eu/EUbookshop.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['114','Europarl-ST','Multi-Lingual','Dataset contains paired audio-text samples for speech translation, constructed using the debates carried out in the European Parliament in the period between 2008 and 2012. Contains 6 Euro languages: German, English, Spanish, French, Italian and Portuguese.','n/a','n/a','Speech Translation','2020','Iranzo-S?nchez et al.','https://www.mllp.upv.es/europarl-st/','https://arxiv.org/pdf/1911.03167.pdf',],
		['115','European Parliament Proceedings (Europarl)','Multi-Lingual','The Europarl parallel corpus is extracted from the proceedings of the European Parliament. It includes versions in 21 European languages.','10M+','XML','Text Corpora, Machine Translation','2002','Koehn et al.','http://statmt.org/europarl/','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['116','Europeana Newspapers','Multi-Lingual','Named Entity Recognition corpora for Dutch, French, German languages from Europeana Newspapers. Data is encoded in the IOB format.','486,218','BIO','Named Entity Recognition (NER)','2016','Neudecker','https://github.com/EuropeanaNewspapers/ner-corpora','http://lab.kb.nl/dataset/europeana-newspapers-ner',],
		['117','Event2Mind','English','Dataset contains 25,000 events and free-form descriptions of their intents and reactions','25,000','CSV','Commonsense Inference','2018','Rashkin et al.','https://uwnlp.github.io/event2mind/','https://arxiv.org/pdf/1805.06939.pdf',],
		['118','Event-focused Emotion Corpora for German and English','English, German','German and English emotion corpora for emotion classification, annotated with crowdsourcing in the style of the ISEAR resources.','2,002','TSV','Text Corpora, Emotion Classification','2019','Troiano et al.','https://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/deisear/','https://arxiv.org/pdf/1905.13618.pdf',],
		['119','Examiner Pseudo-News Corpus','English','Clickbait, spam, crowd-sourced headlines from 2010 to 2015.','3,089,781','CSV','Clustering, Events, Sentiment Analysis','2017','Kulkarni','https://www.kaggle.com/therohk/examine-the-examiner','',],
		['120','Excitement Datasets','English, Italian','Datasets contain negative feedbacks from customers where they state reasons for dissatisfaction with a given company. The datasets are available in English and Italian.','n/a','XML','Classification, Sentiment Analysis','2015','Kotlerman et al.','https://github.com/hltfbk/Excitement-Open-Platform/wiki/Data-Sets','https://pdfs.semanticscholar.org/615b/1353bacacb3774d7dc88a3ce243c84559333.pdf?_ga=2.42297447.354477249.1582324561-958501894.1582324561',],
		['121','Explain Like I?m Five (ELI5)','English','The dataset contains 270K threads of open-ended questions that require multi-sentence answers. It was extracted from subreddit titled ?Explain Like I?m Five? (ELI5), in which an online community answers questions with responses that 5-year-olds can comprehend. Facebook scripts allow you to preprocess data.','270,000','Text','Question Answering, Reading Comprehension','2019','Fan et al.','https://github.com/facebookresearch/ELI5','https://arxiv.org/pdf/1907.09190.pdf',],
		['122','Explanations for Science Questions','English','Data contains: gold explanation sentences supporting 363 science questions, relation annotation for a subset of those explanations, and a graphical annotation tool with annotation guidelines.','1,363','CSV','Question Answering, Reading Comprehension','2016','Jansen et al.','http://ai2-website.s3.amazonaws.com/data/COLING2016_Explanations_Oct2016.zip','https://pdfs.semanticscholar.org/04d7/b7851683809cab561d09b5c5c80bd5c33c80.pdf?_ga=2.42297447.354477249.1582324561-958501894.1582324561',],
		['123','Fact-based Visual Question Answering (FVQA)','English','Dataset contains image question anwering triples','5,826 questions; 2,190 images','JSON','Question Answering, Visual','2017','Wang et al.','https://www.dropbox.com/s/iyz6l7jhbt6jb7q/new_dataset_release.zip?dl=0','https://arxiv.org/pdf/1606.05433.pdf',],
		['124','Fast.AI','','Fast.AI course datasets','','','','','','','',],
		['125','Finlex','Finnish, Swedish','Dataset is a collection of legislative and other judicial information of Finland, which is available in Finnish and Swedish.','7.98M','XML','Text Corpora, Machine Translation','2012','Tiedemann et al.','http://opus.nlpl.eu/Finlex.php','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['126','Finnish News Corpus for Named Entity Recognition','Finnish','Dataset contains 953 articles (193,742 word tokens) with 6 named entity classes: organization, location, person, product, event, and date.','953','CSV','Named Entity Recognition (NER)','2018','G?ng?r &amp; Sohrab et al.','https://github.com/mpsilfve/finer-data','https://www.aclweb.org/anthology/C18-1177.pdf',],
		['127','Fiskm?','Finnish, Swedish','Dataset is a parallel corpus of Finnish and Swedish Languages.','4.24M','XML','Machine Translation','2012','Tiedemann et al.','https://version.helsinki.fi/Helsinki-NLP/fiskmo','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['128','FiveThirtyEight','English','Ppinion poll analysis, politics, economics, and sports blogging. The data and code on Github is behind the stories and interactives at FiveThirtyEight.','','','','','','https://data.fivethirtyeight.com/','',],
		['129','Figure-Eight','English','open datasets created on the Figure Eight platform.','','','','','','https://www.figure-eight.com/data-for-everyone/','',],
		['130','FourSquare','English','FourSquare - NYC Restaurant Rich Dataset (Check-ins, Tips, Tags). Location based social networks have attracted millions of users and massively contains their digital footprints. We have crawled a part of these digital footprints from Foursquare in order to study the problems of personalized location recommendation and search. This dataset includes check-in, tip and tag data of restaurant venues in NYC collected from Foursquare from 24 October 2011 to 20 February 2012. It contains 3112 users and 3298 venues with 27149 check-ins and 10377 tips (written in English).','','','','2012','','https://sites.google.com/site/yangdingqi/home/foursquare-dataset, https://www.kaggle.com/chetanism/foursquare-nyc-and-tokyo-checkin-dataset/home','',],
		['131','FQuAD','French','Dataset contains 25,000+ questions on a set of Wikipedia articles, modeled after SQuAD.','25,000+','JSON','Question Answering, Reading Comprehension','2020','d?Hoffschmidt et al.','https://fquad.illuin.tech/','https://arxiv.org/pdf/2002.06071.pdf',],
		['132','Friendster','English','Friendster','','','','2011','','https://archive.org/download/friendster-dataset-201107','',],
		['133','GAP Coreference Dataset','English','Dataset contains 8,908 gender-balanced coreference-labeled pairs of (ambiguous pronoun, antecedent name), sampled from Wikipedia.','8,908','TSV','Coreference Resolution','2018','Webster et al.','https://github.com/google-research-datasets/gap-coreference','https://arxiv.org/pdf/1810.05201.pdf',],
		['134','GDELT Project','English','GDELT Project monitors the world s broadcast, print, and web news from nearly every corner of every country in over 100 languages and identifies the people, locations, organizations, themes, sources, emotions, counts, quotes, images and events driving our global society every second of every day, creating a free open platform for computing on the entire world.','','','','','','https://www.gdeltproject.org/','',],
		['135','GeoQuery','English','Dataset contains utterances issued to a database of US geographical facts.','877','JSON','Semantic Parsing, Text-to-SQL','2017','Zelle &amp; Iyer et al.','https://github.com/jkkummerfeld/text2sql-data','https://pdfs.semanticscholar.org/1c9d/f99cce1903d34c53025e86e72331bbfbe08f.pdf?_ga=2.119839695.69779999.1583855679-45550878.1577737485',],
		['136','GermEval 2014 NER Shared Task','German','The data was sampled from German Wikipedia and News Corpora as a collection of citations.The dataset covers over 31,000 sentences corresponding to over 590,000 tokens.','31,000+','TSV','Named Entity Recognition','2014','Benikova et al.','https://sites.google.com/site/germeval2014ner/data','https://pdfs.semanticscholar.org/f6ea/166f38804860dc6b4ef66d5dca0eed17a2d1.pdf?_ga=2.84254811.354477249.1582324561-958501894.1582324561',],
		['137','Global Voices Parallel Corpus','Multi-Lingual','Dataset contains news articles from the web site Global Voices in multiple languages.','n/a','Text','Machine Translation','2015','CASMACAT','http://casmacat.eu/corpus/global-voices.html','http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf',],
		['138','Googe Research','English','50,000 relations from Wikipedia. 100,000 feature vectors from YouTube videos. 1.8 million historical infoboxes. 40 million entities derived from webpages. 11 billion Freebase entities in 800 million web documents. 350 billion words? worth from books analyzed for syntax.','','','','','','https://research.google/tools/datasets/, http://googleresearch.blogspot.sg/2013/12/free-language-lessons-for-computers.html','',],
		['139','Google','English','Google Public Data','','','','','','https://www.google.com/publicdata/directory, https://cloud.google.com/bigquery/public-data/','',],
		['140','Google Books N-grams','Multi-Lingual','N-grams from a very large corpus of books.','2.2 TB of text','Text','Classification, Clustering','2011','Google','https://aws.amazon.com/datasets/google-books-ngrams/, https://datahub.io/dataset/google-books-ngram','https://static.googleusercontent.com/media/research.google.com/ro//pubs/archive/38277.pdf',],
		['141','GQA','English','Question answering on image scene graphs.','22M','JSON, H5','Question Answering, Visual, Commonsense','2019','Hudson et al.','https://cs.stanford.edu/people/dorarad/gqa/download.html','https://arxiv.org/pdf/1902.09506.pdf',],
		['142','Groningen Meaning Bank','English','Datasets contains texts in raw and tokenised format, tags for part of speech, named entities and lexical categories, and discourse representation structures compatible with first-order logic.','10,000','XML','Text Corpora','2014','University of Groningen','https://gmb.let.rug.nl/data.php','',],
		['143','Guardian','English','Guardian','','','','','','https://www.theguardian.com/data','',],
		['144','Guttenberg Book Corpus','Multi-Lingual','Dataset contains 60,000 eBooks.','60,000','Text','Text Corpora','1996-2019','Guttenberg','http://www.gutenberg.org/wiki/Gutenberg:Offline_Catalogs','https://arxiv.org/pdf/1812.08092.pdf',],
		['145','Hansards Canadian Parliament','English','Dataset contains pairs of aligned text chunks (sentences or smaller fragments) from the official records (Hansards) of the 36th Canadian Parliament.','1.3M','Text','Text Corpora','2001','Natural Language Group - USC','https://www.isi.edu/natural-language/download/hansard/','',],
		['146','Harvard Library','English','Dataset contains books, journals, electronic resources, manuscripts, archival materials, scores, audio, video and other materials.','12.7M','MODS, Dublin Core','Text Corpora','n/a','Harvard','https://wiki.harvard.edu/confluence/display/LibraryStaffDoc/LibraryCloud','',],
		['147','Hate Speech Identification Dataset','English','Dataset contains lexicons, notebooks containing content that is racist, sexist, homophobic, and offensive in general.','n/a','CSV','Classification','2017','Davidson et al.','https://github.com/t-davidson/hate-speech-and-offensive-language','https://arxiv.org/pdf/1703.04009.pdf',],
		['148','HellaSwag','English','Dataset for studying grounded commonsense inference. It consists of 70k multiple choice questions about grounded situations: each question comes from one of two domains -- activitynet or wikihow -- with four answer choices about what might happen next in the scene.','70,000','JSON','Commonsense Reasoning','2019','Zellers et al.','https://rowanzellers.com/hellaswag/','https://arxiv.org/pdf/1905.07830.pdf',],
		['149','Historical Newspapers Daily Word Time Series Dataset','English','Dataset contains daily contents of newspapers published in the US and UK from 1836 to 1922.','25,000','n/a','Text Corpora','2017','Dzogang et al.','https://datadryad.org/stash/dataset/doi:10.5061/dryad.nh775','https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0165736&amp;type=printable',],
		['150','Home Depot Product Search Relevance','English','Dataset contains a number of products and real customer search terms from Home Depot&#39;s website.','n/a','CSV','Classification','2015','Home Depot','https://www.kaggle.com/c/home-depot-product-search-relevance/data','',],
		['151','HotpotQA','English','Dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems.','1.25M','JSON','Question Answering, Reading Comprehension','2018','Yang et al.','https://hotpotqa.github.io/','https://arxiv.org/pdf/1809.09600.pdf',],
		['152','How2','English, Portuguese','Dataset of instructional videos covering a wide variety of topics across video clips (about 2,000 hours), with word-level time alignments to the ground-truth English subtitles. And 300 hours was translated into Portuguese subtitles.','~2,000 Hours','n/a','Speech-to-Text, Translation, Summarization, Visual','2018','Sanabria et al.','https://github.com/srvk/how2-dataset','https://arxiv.org/pdf/1811.00347.pdf',],
		['153','Human-in-the-loop Dialogue Simulator (HITL)','English','Dataset provides a framework for evaluating a bot?s ability to learn to improve its performance in an online setting using feedback from its dialog partner. The dataset contains questions based on the bAbI and WikiMovies datasets, with the addition of feedback from the dialog partner.','n/a','Text','Question Answering, Reading Comprehension','2016','Li et al.','https://research.fb.com/downloads/babi/','https://arxiv.org/pdf/1611.09823.pdf',],
		['154','IIT Bombay English-Hindi Corpus','English, Hindi','Dataset contains parallel corpus for English-Hindi as well as monolingual Hindi corpus collected from a variety of existing sources.','1.49M','n/a','Machine Translation','2018','Kunchukuttan et al.','http://www.cfilt.iitb.ac.in/iitb_parallel/','https://arxiv.org/pdf/1710.02855.pdf',],
		['155','Indic Languages Multilingual Parallel Corpus','Indian','Dataset contains several languages: Bengali, Hindi, Malayalam, Tamil, Telugu, Sinhalese, Urdu and English. The corpus has been collected from OPUS and belongs to the spoken language (OpenSubtitles) domain.','n/a','Tar','Machine Translation','2018','NICT &amp; Kyoto Univ.','http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/index.html','https://www.aclweb.org/anthology/Y18-3003.pdf',],
		['156','InsuranceQA','English','Dataset contains questions and answers collected from the website Insurance Library. It consists of questions from real world users, the answers with high quality were composed by professionals with deep domain knowledge. There are 16,889 questions in total.','16,889','n/a','Question Answering, Reading Comprehension','2015','Feng et al.','https://github.com/shuzi/insuranceQA','https://arxiv.org/pdf/1508.01585.pdf',],
		['157','Irony Sarcasm Analysis Corpus','English','Dataset contains tweets in 4 subgroups: irony, sarcasm, regular and figurative. Requires using Twitter API in order to obtain tweets.','33,000','TSV','Classification, Sentiment Analysis','2016','Ling et al.','http://romanklinger.de/ironysarcasm/','http://www.roman-klinger.de/publications/ling2016.pdf',],
		['158','IWSLT 15 English-Vietnamese','Multi-Lingual','Sentence pairs for translation.','133,000','Text','Machine Translation','2015','Stanford','https://nlp.stanford.edu/projects/nmt/','https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf',],
		['159','IWSLT&#39;15 English-Vietnamese ','Multi-Lingual','Parallel corpus used for machine translation English-Vietnamese.','~130,000','Text','Machine Translation','2015','Hong et al.','https://nlp.stanford.edu/projects/nmt/','https://workshop2015.iwslt.org/downloads/IWSLT_2015_EP_3.pdf',],
		['160','Jeapardy Questions Answers','English','Dataset contains Jeopardy questions, answers and other data.','216,930','JSON','Question Answering, Reading Comprehension','2014','Anonymous','https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/','https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file/',],
		['161','Kensho Derived Wikimedia Dataset (KDWD)','English','Dataset contains two main components - a link annotated corpus of English Wikipedia pages and a compact sample of the Wikidata knowledge base.','n/a','CSV, JSON','Text Corpora, Knowledge Base','2020','Kensho R&amp;D','https://www.kaggle.com/kenshoresearch/kensho-derived-wikimedia-data','',],
		['162','Khaleej-2004 Corpus','Arabic','Dataset contains more than 5,000 articles which correspond to nearly 3 millions words across 4 topics:  International News, Local News, Economy, and Sports.','5,690','HTML','Text Corpora','2004','Abbas et al.','https://sites.google.com/site/mouradabbas9/corpora','',],
		['163','KorQuAD','Korean','Dataset containing a total of 100,000+ question answer pairs.','102,960','JSON','Question Answering, Reading Comprehension','2019','Lim et al.','https://korquad.github.io/','https://arxiv.org/ftp/arxiv/papers/1909/1909.07005.pdf',],
		['164','Language Modeling Broadened to Account for Discourse Aspects (LAMBADA)','English','Dataset contains narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word.','10,022','Text','Natural Language Understanding, Language Modeling','2016','Paperno et al.','https://zenodo.org/record/2630551','https://arxiv.org/pdf/1606.06031.pdf',],
		['165','Large Movie Review Dataset - Imdb','English','Dataset contains 25,000 highly polar movie reviews for training, and 25,000 for testing','50,000','Text','Classification, Sentiment Analysis','2011','Maas et al.','http://ai.stanford.edu/~amaas/data/sentiment/','https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf',],
		['166','LionBridge','Chinese','Chinese Treebank: This treebank contains 1.5 million words of annotated and parsed text from Chinese news, government documents, and magazine articles.','1.5M','Text','','','','https://lionbridge.ai/datasets/14-best-chinese-language-datasets-for-machine-learning/','',],
		['167','LC-QuAD 2.0','English','Dataset contains questions and SPARQL queries. LC-QuAD uses DBpedia v04.16 as the target KB.','30,000','JSON','Question Answering, Knowledge Graph','2017','Dubey et al.','http://lc-quad.sda.tech/','http://jens-lehmann.org/files/2019/iswc_lcquad2.pdf',],
		['168','Legal Case Reports','English','Federal Court of Australia cases from 2006 to 2009.','4,000','Text','Classification','2012','Galgani et al.','https://archive.ics.uci.edu/ml/machine-learning-databases/00239/','https://www.aclweb.org/anthology/W12-0515.pdf',],
		['169','LibriSpeech ASR','English','Large-scale (1000 hours) corpus of read English speech.','n/a','FLAC','Speech Recognition','2015','OpenSLR','http://www.openslr.org/12/','http://www.danielpovey.com/files/2015_icassp_librispeech.pdf',],
		['170','LibriVoxDeEn','German, English','Dataset contains sentence-aligned triples of German audio, German text, and English translation, based on German audio books. The corpus consists of over 100 hours of audio material and over 50k parallel sentences.','50,000+','Text, TSV','Speech Translation, Machine Translation','2019','Beilharz et al.','https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/TMEDTX','https://arxiv.org/pdf/1910.07924.pdf',],
		['171','Ling-Spam Dataset','English','Corpus contains both legitimate and spam emails.','n/a','Text','Classification','2000','Androutsopoulos et al.','https://aclweb.org/aclwiki/Spam_filtering_datasets','http://www2.aueb.gr/users/ion/docs/ir_memory_based_antispam_filtering.pdf',],
		['172','LitBank','English','Dataset contains 100 works of English-language fiction. It currently contains annotations for entities, events and entity coreference in a sample of ~2,000 words from each of those texts, totaling 210,532 tokens.','100','TSV, Text','Named Entity Recognition','2019','Bamman et al.','https://github.com/dbamman/litbank','https://arxiv.org/pdf/1912.01140.pdf',],
		['173','Meta-Learning Wizard-of-Oz (MetaLWOz)','English','Dataset designed to help develop models capable of predicting user responses in unseen domains. It was created by crowdsourcing 37,884 goal-oriented dialogs, covering 227 tasks in 47 domains.','37,884','Text','Dialogue','2019','Microsoft','https://www.microsoft.com/en-us/research/project/metalwoz/','https://arxiv.org/pdf/1908.05854.pdf',],
		['174','Microsoft Information-Seeking Conversation (MISC) dataset','English','Dataset contains recordings of information-seeking conversations between human ?seekers? and ?intermediaries?. It includes audio and video signals; transcripts of conversation; affectual and physiological signals; recordings of search and other computer use; and post-task surveys on emotion, success, and effort.','n/a','various','Speech Recognition, Dialogue, Visual','2018','Microsoft','https://www.microsoft.com/en-us/download/details.aspx?id=55594','https://arxiv.org/pdf/1804.08759.pdf',],
		['175','Microsoft Machine Reading COmprehension Dataset (MS MARCO)','English','Dataset focused on machine reading comprehension, question answering, and passage ranking, keyphrase extraction, and conversational search studies.','1,010,916','JSON','Question Answering, Reading Comprehension','2016','Bajaj et al.','http://www.msmarco.org/leaders.aspx','https://arxiv.org/pdf/1611.09268.pdf',],
		['176','Microsoft News QA ','English','The purpose of the NewsQA dataset is to help the research community build algorithms that are capable of answering questions requiring human-level comprehension and reasoning skills. Leveraging CNN articles from the DeepMind Q&A Dataset, we prepared a crowd-sourced machine reading comprehension dataset of 120K Q&A pairs.','','','','','','https://www.microsoft.com/en-us/research/project/newsqa-dataset/','',],
		['177','Microsoft Research Paraphrase Corpus','English','Dataset contains pairs of sentences which have been extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship.','5,800','Text','Paraphrasing','2005','Dolan et al.','https://www.microsoft.com/en-us/download/details.aspx?id=52398','https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/I05-50025B15D.pdf',],
		['178','Microsoft Research Social Media Conversation Corpus','English','A-B-A triples extracted from Twitter.','4,232','Text','Graph Analysis','2016','Sordoni et al.','https://www.microsoft.com/en-us/download/details.aspx?id=52375','https://www.microsoft.com/en-us/research/wp-content/uploads/2015/06/N15-1020.pdf',],
		['179','Microsoft Speech Corpus','Indian','Dataset contains conversational and phrasal speech training and test data for Telugu, Tamil and Gujarati languages.','n/a','Wav','Speech Recognition','2019','Microsoft','https://msropendata.com/datasets/7230b4b1-912d-400e-be58-f84e0512985e','',],
		['180','Microsoft Speech Language Translation Corpus (MSLT)','Multi-Lingual','Dataset contains conversational, bilingual speech test and tuning data for English, Chinese, and Japanese. It includes audio data, transcripts, and translations; and allows end-to-end testing of spoken language translation systems on real-world data.','n/a','Wav','Speech Recognition, Machine Translation','2017','Federmann et al.','https://www.microsoft.com/en-us/download/details.aspx?id=55951','https://workshop2016.iwslt.org/downloads/IWSLT_2016_paper_12.pdf',],
		['181','MovieLens','English','Dataset contains 22,000,000 ratings and 580,000 tags applied to 33,000 movies by 240,000 users.','~22M','Text','Clustering, Classification, Regression','2016','Harper et al.','https://grouplens.org/datasets/movielens/','http://files.grouplens.org/papers/harper-tiis2015.pdf',],
		['182','MovieTweetings','English','Movie rating dataset based on public and well-structured tweets.','822,784','Text','Classification, Regression','2018','Dooms','https://github.com/sidooms/MovieTweetings/tree/master/latest','http://crowdrec2013.noahlab.com.hk/papers/crowdrec2013_Dooms.pdf',],
		['183','MSParS','English','Dataset for the open domain semantic parsing task.','81,826','Satori','Semantic Parsing','2019','Microsoft','https://github.com/msra-nlc/MSParS','http://tcci.ccf.org.cn/conference/2019/papers/EV20.pdf',],
		['184','Multi30k','German, English','Dataset of images paired with sentences in English and German. This dataset extends the Flickr30K dataset.','31,014','n/a','Machine Translation, Multi-Modal Learning','2016','Elliott et al.','https://github.com/multi30k/dataset','https://arxiv.org/pdf/1605.00459.pdf',],
		['185','Multi-Domain Wizard-of-Oz Dataset (MultiWoz)','English','Dataset of human-human written conversations spanning over multiple domains and topics. The dataset was collected based on the Wizard of Oz experiment on Amazon MTurk.','10,438','JSON','Dialogue','2018','Budzianowski et al.','https://www.repository.cam.ac.uk/handle/1810/280608','https://arxiv.org/pdf/1810.00278.pdf',],
		['186','MultiLing Pilot 2011 Dataset','Multi-Lingual','Dataset is derived from publicly available WikiNews English texts and translated into 7 languages: Arabic, Czech, English, French, Greek, Hebrew, Hindi.','n/a','Text','Summarization','2011','Giannakopoulos et al.','https://github.com/ayushoriginal/Multi-Document-Summarization/tree/master/TAC%202011%20MultiLing%20Pilot%20Dataset','https://pdfs.semanticscholar.org/634a/1db729f005052dff40016fabcc60b7c2fc84.pdf?_ga=2.264263951.1631655770.1585325522-45550878.1577737485',],
		['187','Multilingual Corpus of Sentence-Aligned Spoken Utterances (MaSS)','Multi-Lingual','Dataset of 8,130 parallel spoken utterances across 8 languages (56 language pairs). Languages: Basque, English, Finnish, French. Hungarian, Romanian, Russian, Spanish.','8,130','n/a','Speech Corpora','2020','Boito et al.','https://github.com/getalp/mass-dataset','https://arxiv.org/pdf/1907.12895.pdf',],
		['188','MultiLingual Question Answering (MLQA)','Multi-Lingual','Dataset for evaluating cross-lingual question answering performance. ~12K QA instances in English and 5K in each other language in SQuAD format in seven languages - English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese.','46,444','JSON','Question Answering, Reading Comprehension','2019','Lewis et al.','https://github.com/facebookresearch/MLQA','https://arxiv.org/pdf/1910.07475.pdf',],
		['189','Multimodal Comprehension of Cooking Recipes (RecipeQA)','English','Dataset for multimodal comprehension of cooking recipes. It consists of over 36K question-answer pairs automatically generated from approximately 20K unique recipes with step-by-step instructions and images.','20,000','JSON','Question Answering, Reading Comprehension','2018','Yagcioglu et al.','https://hucvl.github.io/recipeqa/','https://www.aclweb.org/anthology/D18-1166.pdf',],
		['190','MultiNLI Matched/Mismatched','English','Dataset contains sentence pairs annotated with textual entailment information.','433,000','JSON, Text','Entailment','2017','Williams et al.','http://www.nyu.edu/projects/bowman/multinli/','https://arxiv.org/pdf/1707.08172.pdf',],
		['191','MuST-C','Multi-Lingual','Dataset is a speech translation corpus containing 385 hours from Ted talks for speech translation from English into several languages: Dutch, French, German, Italian, Portuguese, Romanian, Russian, &amp; Spanish. Requires filling request form.','385 Hours','n/a','Speech Translation','2019','Di Gangi et al.','https://ict.fbk.eu/must-c/','https://www.aclweb.org/anthology/N19-1202.pdf',],
		['192','MutualFriends','English','Task where two agents must discover which friend of theirs is mutual based on the friend&#39;s attributes.','n/a','JSON','Dialogue','2017','He et al.','https://stanfordnlp.github.io/cocoa/','https://arxiv.org/pdf/1704.07130.pdf',],
		['193','Named Entity Model for German, Politics (NEMGP)','German','Dataset contains texts from Wikipedia and WikiNews, manually annotated with named entity information.','5,094','Text','Named Entity Recognition (NER)','2013','Zastrow','https://www.thomas-zastrow.de/nlp/','https://www.thomas-zastrow.de/nlp/',],
		['194','NarrativeQA','English','Dataset contains the list of documents with Wikipedia summaries, links to full stories, and questions and answers.','1,572','CSV','Question Answering, Reading Comprehension','2017','Kocisk? et al.','https://github.com/deepmind/narrativeqa','https://arxiv.org/pdf/1712.07040.pdf',],
		['195','Natural Questions (NQ)','English','Dataset contains questions from real users, and it requires QA systems to read and comprehend an entire Wikipedia article that may or may not contain the answer to the question.','320,000+','HTML','Question Answering, Reading Comprehension','2019','Kwiatkowski et al.','https://ai.google.com/research/NaturalQuestions','https://persagen.com/files/misc/kwiatkowski2019natural.pdf',],
		['196','Neutralizing Biased Text','English','A parallel corpus of 180,000+ sentence pairs where one sentence is biased and the other is neutralized. The data were obtained from debiasing wikipedia edits.','180,000','n/a','Biased Text Neutralization','2019','Pryzant et al.','https://github.com/rpryzant/neutralizing-bias','https://arxiv.org/pdf/1911.09709.pdf',],
		['197','New York Times','English','The New York Times Annotated Corpus','','','','','','https://catalog.ldc.upenn.edu/LDC2008T19, https://code.google.com/p/nyt-salience/','',],
		['198','News Headlines Dataset for Sarcasm Detection','English','High quality dataset with Sarcastic and Non-sarcastic news headlines.','26,709','JSON','Clustering, Events, Language Detection','2018','Misra','https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection','https://arxiv.org/pdf/1908.07414.pdf',],
		['199','News Headlines Of India','English','Dataset contains archive of noteable events in India during 2001-2018, recorded by the Times of India.','2,969,922','CSV','Text Corpora','2017','Kaggle','https://www.kaggle.com/therohk/india-headlines-news-dataset','https://www.kaggle.com/therohk/india-headlines-news-dataset/version/2',],
		['200','NewsQA','English','Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN.','12,744','JSON, CSV','Question Answering, Reading Comprehension','2017','Trischler et al.','https://github.com/Maluuba/newsqa','https://arxiv.org/pdf/1611.09830.pdf',],
		['201','NLP Chinese Corpus','Chinese','Large text corpora in Chinese.','10M+','JSON','Text Corpora','2019','Xu et al.','https://github.com/brightmart/nlp_chinese_corpus','https://bmcmedinformdecismak.biomedcentral.com/track/pdf/10.1186/s12911-019-0770-7',],
		['202','NPS Chat Corpus','English','Posts from age-specific online chat rooms.','~500,000','XML','Dialogue','2007','Forsyth et al.','https://www.kaggle.com/nltkdata/nps-chat','https://core.ac.uk/download/pdf/36731948.pdf',],
		['203','NUS SMS Corpus','Mandarin, English','SMS messages collected between 2 users, with timing analysis.','67,093','XML','Dialogue','2013','Kan et al.','https://www.kaggle.com/rtatman/the-national-university-of-singapore-sms-corpus','https://link.springer.com/article/10.1007%2Fs10579-012-9197-9',],
		['204','NYSK Dataset','English','English news articles about the case relating to allegations of sexual assault against the former IMF director Dominique Strauss-Kahn.','10,421','XML','Sentiment Analysis, Topic Extraction','2013','Dermouche et al.','https://archive.ics.uci.edu/ml/machine-learning-databases/00260/','',],
		['205','One Week of Global News Feeds','Multi-Lingual','Dataset contains most of the new news content published online over one week in 2017 and 2018.','3.3M','CSV','Text Corpora','2018','Kulkarni et al.','https://www.kaggle.com/therohk/global-news-week','https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ILAT5B',],
		['206','OneCommon','English','Dataset contains 6,760 dialogues.','6,760','JSON','Dialogue','2019','Udagawa et al.','https://github.com/Alab-NII/onecommon','https://arxiv.org/pdf/1907.03399.pdf',],
		['207','OneSeC Small','Multi-Lingual','Automatically-generated corpora in multiple languages with sense annotations for nouns using WordNet for English and BabelNet for all other languages as inventories of senses.','1M+','XML','Word Sense Disambiguation ','2019','Scarlini et al.','http://trainomatic.org/data/onesec.tar.gz','http://www.trainomatic.org/data/ACL_2019_Scarlinietal.pdf',],
		['208','OntoNotes 5.0','Multi-Lingual','Dataset contains various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).','n/a','Text, SQL','Information Retrieval, Syntactic Parsing','2013','Weischedel et al.','https://catalog.ldc.upenn.edu/LDC2013T19','https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf',],
		['209','Open Images V6','English','Dataset containing millions of images that have been annotated with image-level labels and object bounding boxes.','9,178,275','TSV, CSV','Automatic Image Captioning','2018','Kuznetsova et al.','https://storage.googleapis.com/openimages/web/download.html','https://arxiv.org/pdf/1811.00982.pdf',],
		['210','Open Research Corpus','English','Dataset contains over 39 million published research papers in Computer Science, Neuroscience, and Biomedical.','39M','JSON','Text Corpora','2018','Ammar et al.','https://api.semanticscholar.org/corpus/','https://arxiv.org/pdf/1804.09635.pdf',],
		['211','Open Super-Large Crawled Almanach Corpus (OSCAR)','Multi-Lingual','Multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the goclassy architecture.166 different languages available.','n/a','Text','Text Corpora','2019','Su?rez et al.','https://traces1.inria.fr/oscar/','https://hal.inria.fr/hal-02148693/document',],
		['212','OpenBookQA','English','Dataset modeled after open book exams for assessing human understanding of a subject. It consists of 5,957 multiple-choice elementary-level science questions (4,957 train, 500 dev, 500 test), which probe the understanding of a small &#34;book&#34; of 1,326 core science facts and the application of these facts to novel situations.','5,957','JSON','Question Answering, Reading Comprehension','2018','Mihaylov et al.','https://leaderboard.allenai.org/open_book_qa/submissions/get-started','https://arxiv.org/pdf/1809.02789.pdf',],
		['213','OpenSubtitles','Multi-Lingual','Dataset of multi-lingual dialogs from movie scripts. Includes 62 languages.','n/a','XML, XCES','Dialogue','2016','Tiedemann et al.','http://opus.nlpl.eu/OpenSubtitles2018.php','http://www.lrec-conf.org/proceedings/lrec2016/pdf/947_Paper.pdf',],
		['214','OpenWebTextCorpus','English','Dataset contains millions of webpages text stemming from reddit urls totalling 38Gb of text data.','8,013,769','n/a','Text Corpora','2019','Radford et al.','https://skylion007.github.io/OpenWebTextCorpus/','https://skylion007.github.io/OpenWebTextCorpus/',],
		['215','OpinRank Review Dataset','English','Reviews of cars and hotels from Edmunds.com and TripAdvisor.','Edmunds: 42,230, TripAdivsor: 259,000','Text','Information Retrieval, Entity Ranking, Entiry Retrieval','2011','Ganesan et al.','http://archive.ics.uci.edu/ml/machine-learning-databases/00205/','http://kavita-ganesan.com/opinion-based-entity-ranking/#.Xme-5KhKiUk',],
		['216','Parallel Arabic DIalectal Corpus (PADIC)','Arabic','Dataset is a multi-dialectal corpus -  contains six dialects in addition to MSA in Buckwalter format.','6,000+','HTML','Text Corpora','2013','Abbas et al.','https://sites.google.com/site/torjmanepnr/6-corpus','',],
		['217','Parallel Meaning Bank','Multi-Lingual','Dataset contains sentences and texts in raw and tokenised format, syntactic analysis, word senses, thematic roles, reference resolution, and formal meaning representations. The annotated parallel corpus inclues English, German, Dutch and Italian languages.','8,705','XML','Text Corpora','2017','University of Groningen','https://pmb.let.rug.nl/data.php','https://www.aclweb.org/anthology/E17-2039.pdf',],
		['218','Paraphrase Adversaries from Word Scrambling (PAWS)','English','Dataset contains 108,463 human-labeled and 656k noisily labeled pairs that feature the importance of modeling structure, context, and word order information for the problem of paraphrase identification.','750,000+','TSV','Paraphrasing Identification','2019','Zhang et al.','https://github.com/google-research-datasets/paws','https://arxiv.org/pdf/1904.01130.pdf',],
		['219','Paraphrase Adversaries from Word Scrambling (PAWS-X)','Multi-Lingual','Dataset contains 23,659 human translated PAWS evaluation pairs and 296,406 machine translated training pairs in six typologically distinct languages: French, Spanish, German, Chinese, Japanese, and Korean. All translated pairs are sourced from examples in PAWS-Wiki.','300,000+','TSV','Paraphrasing Identification','2019','Yang et al.','https://github.com/google-research-datasets/paws/tree/master/pawsx','https://arxiv.org/pdf/1908.11828.pdf',],
		['220','Paraphrase and Semantic Similarity in Twitter (PIT)','English','Dataset focuses on whether tweets have (almost) same meaning/information or not.','18,762','Text','Classification','2015','Xu et al.','https://github.com/cocoxu/SemEval-PIT2015','https://www.aclweb.org/anthology/S15-2001.pdf',],
		['221','ParCorFull','German, English','A parallel corpus annotated for the task of translation of corefrence across languages.','14,927','XML','Machine Translation, Coreference Resolution','2018','Lapshinova-Koltunski et al.','https://lindat.mff.cuni.cz/repository/xmlui/handle/11372/LRT-2614','https://www.aclweb.org/anthology/L18-1065.pdf',],
		['222','Personae Corpus','Dutch','Collected for experiments in Authorship Attribution and Personality Prediction. Consists of 145 Dutch-language essays.','145','Text','Classification, Regression','2008','Luyckx et al.','https://www.clips.uantwerpen.be/datasets/personae-corpus','http://www.lrec-conf.org/proceedings/lrec2008/pdf/759_paper.pdf',],
		['223','Personalized Dialog','English','Dataset of dialogs from movie scripts.','12,000','Text','Dialogue','2017','Joshi et al.','https://github.com/chaitjo/personalized-dialog','https://arxiv.org/pdf/1706.07503.pdf',],
		['224','PG-19','English','Dataset contains a set of books extracted rom the Project Gutenberg books library, that were published before 1919. It also contains metadata of book titles and publication dates.','28,752','Text','Text Corpora, Language Modeling','2019','Rae et al.','https://github.com/deepmind/pg19','https://arxiv.org/pdf/1911.05507.pdf',],
		['225','Physical IQA','English','Dataset is used for commonsense QA benchmark for naive physics reasoning focusing on how we interact with everyday objects in everyday situations. The dataset includes 20,000 QA pairs that are either multiple-choice or true/false questions.','20,000','JSON','Question Answering, Commonsense','2019','Bisk et al.','https://leaderboard.allenai.org/physicaliqa/submissions/get-started','https://arxiv.org/pdf/1911.11641.pdf',],
		['226','Plaintext Jokes','English','208,000 jokes in this database scraped from three sources.','208,000','JSON','Text Corpora','2016','Pungas et al.','https://github.com/taivop/joke-dataset','',],
		['227','Portuguese Newswire Corpus','Portuguese (Brazil)','Dataset contains x number of newswire articles collected between years 1994-2016. Requires preprocesing of HTML pages, found in GitHub in the download link.','n/a','HTML','Text Corpora','2016','Bogazi?i University','http://mann.cmpe.boun.edu.tr/folha_data/','',],
		['228','Portuguese SQuAD v1.1','Portuguese','Portuguese translation of the SQuAD dataset. The translation was performed using the Google Cloud API.','~100,000','JSON','Question Answering, Reading Comprehension','2019','Carvalho et al.','https://github.com/nunorc/squad-v1.1-pt','',],
		['229','ProPara Dataset','English','Dataset is used for comprehension of simple paragraphs describing processes, e.g., photosynthesis. The comprehension task relies on predicting, tracking, and answering questions about how entities change during the process.','488 ','Google Sheets','Question Answering, Reading Comprehension','2018','Mishra et al.','http://data.allenai.org/propara/','https://arxiv.org/pdf/1805.06975.pdf',],
		['230','QASC','English','QASC is a question-answering dataset with a focus on sentence composition. It consists of 9,980 8-way multiple-choice questions about grade school science (8,134 train, 926 dev, 920 test), and comes with a corpus of 17M sentences.','9,980','JSON','Question Answering, Reading Comprehension','2020','Khot et al.','https://leaderboard.allenai.org/qasc/submissions/get-started','https://arxiv.org/pdf/1910.11473.pdf',],
		['231','QA-SRL Bank','English','Dataset contains question answer pairs for 64,000 sentences. Dataset is used to train model for semantic role labeling','64,000','JSON','Question Answering, Semantic Role Labeling','2018','FitzGerald et al.','http://qasrl.org/','https://arxiv.org/pdf/1805.05377v1.pdf',],
		['232','QA-ZRE','English','Dataset contain question answer pairs with each instance containing a relation, a question, a sentence, and an answer set.','30M','Text','Question Answering, Relation Extraction','2017','Levy et al.','http://nlp.cs.washington.edu/zeroshot/','https://arxiv.org/pdf/1706.04115.pdf',],
		['233','QuaRel Dataset','English','Dataset contains 2,771 story questions about qualitative relationships.','2,771','JSON','Question Answering, Reading Comprehension','2018','Tajford et al.','http://data.allenai.org/quarel/','https://arxiv.org/pdf/1811.08048.pdf',],
		['234','QuaRTz Dataset','English','Dataset contains 3,864 questions about open domain qualitative relationships. Each question is paired with one of 405 different background sentences (sometimes short paragraphs).','3,864','JSON','Question Answering, Reading Comprehension','2019','Tajford et al.','http://data.allenai.org/quartz/','https://arxiv.org/pdf/1909.03553.pdf',],
		['235','Quasar-S &amp; T','English','The Quasar-S dataset consists of 37,000 cloze-style queries constructed from definitions of software entity tags on the popular website Stack Overflow. The Quasar-T dataset consists of 43,000 open-domain trivia questions and their answers obtained from various internet sources.','80,000','JSON','Question Answering, Reading Comprehension','2017','Dhingra et al.','http://curtis.ml.cmu.edu/datasets/quasar/','https://arxiv.org/pdf/1707.03904.pdf',],
		['236','Question Answering in Context (QuAC)','English','Dataset for modeling, understanding, and participating in information seeking dialog.','14,000','JSON','Question Answering, Reading Comprehension','2018','Choi et al.','http://quac.ai/','https://arxiv.org/pdf/1808.07036.pdf',],
		['237','Question NLI','English','Dataset converts SQuAD dataset into sentence pair classification by forming a pair between each question and each sentence in the corresponding context.','110,000','JSON','Inference','2018','Rajpurkar et al.','https://rajpurkar.github.io/SQuAD-explorer/','https://arxiv.org/pdf/1806.03822.pdf',],
		['238','Quora Question Pairs','English','The task is to determine whether a pair of questions are semantically equivalent.','400,000','TSV','Semantic Similarity','2017','Quora','https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs','',],
		['239','Quoref','English','Dataset which tests the coreferential reasoning capability of reading comprehension systems. In this span-selection benchmark containing 24K questions over 4.7K paragraphs from Wikipedia, a system must resolve hard coreferences before selecting the appropriate span(s) in the paragraphs for answering questions.','24,000','JSON','Question Answering, Reading Comprehension','2019','Dasigi et al.','https://leaderboard.allenai.org/quoref/submissions/get-started','https://arxiv.org/pdf/1908.05803.pdf',],
		['240','ReAding Comprehension Dataset From Examinations (RACE)','English','Dataset was collected from the English exams evaluating the students&#39; ability in understanding and reasoning.','28,000','JSON','Question Answering, Reading Comprehension','2017','Lai et al.','http://www.cs.cmu.edu/~glai1/data/race/','https://arxiv.org/pdf/1704.04683.pdf',],
		['241','Reading Comprehension over Multiple Sentences (MultiRC)','English','Dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph.','~10,000','JSON','Question Answering, Reading Comprehension','2018','Khashabi et al.','https://cogcomp.seas.upenn.edu/multirc/','https://www.aclweb.org/anthology/N18-1023.pdf',],
		['242','Reading Comprehension with Commonsense Reasoning Dataset (Record)','English','Reading comprehension dataset which requires commonsense reasoning. Contains 120,000+ queries from 70,000+ news articles.','70,000+','JSON','Question Answering, Reading Comprehension','2018','Zhang et al.','https://sheng-z.github.io/ReCoRD-explorer/','https://arxiv.org/pdf/1810.12885.pdf',],
		['243','Reading Comprehension with Multiple Hops (Qangaroo)','English','Reading Comprehension datasets focussing on multi-hop (alias multi-step) inference. There are 2 datasets: Wikihop (based on wikipedia) and Medhop (based on PubMed research papers).','~53,000','JSON','Question Answering, Reading Comprehension','2018','Welbl et al.','http://qangaroo.cs.ucl.ac.uk/index.html','https://transacl.org/ojs/index.php/tacl/article/viewFile/1325/299',],
		['244','Recognizing Textual Entailment (RTE)','English','Datasets are combined and converted to two-class classification: entailment and not_entailment.','n/a','JSON','Entailment','2006-2009','Dagan et al, Bar Haim et al, Giampiccolo, and Bentivogli et al.','https://dl.fbaipublicfiles.com/glue/superglue/data/v2/RTE.zip','http://u.cs.biu.ac.il/~nlp/RTE2/Proceedings/01.pdf',],
		['245','Reddit All Comments Corpus','English','All Reddit comments (as of 2017).','3,329,219,008','JSON','Text Corpora','2017','Reddit','https://www.reddit.com/r/datasets/comments/6mvrb5/reddit_june_2017_comments_are_now_available/','',],
		['246','Relation Extraction Corpus','English','A human-judged dataset of two relations involving public figures on Wikipedia: about 10,000 examples of &#34;place of birth&#34; and 40,000 examples of &#34;attended or graduated from an institution.&#34;','10,000','JSON','Relation Extraction','2013','Google','https://github.com/google-research-datasets/relation-extraction-corpus','https://ai.googleblog.com/2013/04/50000-lessons-on-how-to-read-relation.html',],
		['247','Relationship and Entity Extraction Evaluation Dataset (RE3D)','English','Entity and Relation marked data from various news and government sources.','n/a','JSON','Classification, Entity and Relation Recognition','2017','Dstl','https://github.com/dstl/re3d','',],
		['248','Restaurants','English','Dataset contains user questions about restaurants, their food types, and locations.','378','JSON','Semantic Parsing, Text-to-SQL','2012','Tang/Popescu/','https://github.com/jkkummerfeld/text2sql-data','https://www.researchgate.net/publication/270878355_Translating_Questions_to_SQL_Queries_with_Generative_Parsers_Discriminatively_Reranked',],
		['249','Reuters Corpora (RCV1, RCV2, TRC2)','English','Large collection of Reuters News stories for use in research and development of natural language processing, information retrieval, and machine learning systems. This corpus, known as "Reuters Corpus, Volume 1" or RCV1, is significantly larger than the older, well-known Reuters-21578 collection heavily used in the text classification community. Reuters Corpora (RCV1, RCV2, TRC2)','','','','','','http://trec.nist.gov/data/reuters/reuters.html','',],
		['250','Reuters News Wire Headline','English','Dataset contains 11 years of timestamped events published on the news-wire.','16,121,310','TSV','Clustering, Events, Language Detection','2018','Kulkarni','https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/XDB74W','',],
		['251','ReVerb45k, Base and Ambiguous','English','3 Datasets. In total, there are 91K triples.','91,000','JSON','Information Retrieval, Knowledge Base','2018','Vashishth et al.','https://github.com/malllabiisc/cesi','https://malllabiisc.github.io/publications/papers/cesi_www18.pdf',],
		['252','Saudi Newspapers Corpus','Arabic','Dataset contains 31,030 Arabic newspaper articles.','31,030','JSON','Text Corpora','2015','Alhagri','https://github.com/ParallelMazen/SaudiNewsNet/tree/master/dataset','',],
		['253','SberQuAD','Russian','Dataset consists of a question answers modeleld after SQuAD.','50,364','CSV','Question Answering, Reading Comprehension','2019','Efimov et al.','https://github.com/sberbank-ai/data-science-journey-2017','https://arxiv.org/pdf/1912.09723.pdf',],
		['254','Schema-Guided Dialogue State Tracking (DSTC 8)','English','Dataset contains 18K dialogues between a virtual assistant and a user.','~18,000','JSON','Dialogue State Tracking','2019','Rastogi et al.','https://github.com/google-research-datasets/dstc8-schema-guided-dialogue','https://arxiv.org/pdf/1909.05855.pdf',],
		['255','Scholar','English','User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct.','817','JSON','Semantic Parsing, Text-to-SQL','2017','Iyer et al.','https://github.com/jkkummerfeld/text2sql-data','https://www.aclweb.org/anthology/P17-1089.pdf',],
		['256','SciQ Dataset','English','Dataset contains 13,679 crowdsourced science exam questions about Physics, Chemistry and Biology, among others. The questions are in multiple-choice format with 4 answer options each.','13,769','JSON','Question Answering, Reading Comprehension','2017','Welbl et al.','http://data.allenai.org/sciq/','https://arxiv.org/pdf/1707.06209.pdf',],
		['257','SciTail Dataset','English','Dataset is an entailment dataset created from multiple-choice science exams and web sentences. Each question and the correct answer choice are converted into an assertive statement to form the hypothesis.','27,026','SNLI, TSV, DGEM','Entailment','2018','Khot et al.','http://data.allenai.org/scitail/','http://ai2-website.s3.amazonaws.com/publications/scitail-aaai-2018_cameraready.pdf',],
		['258','SearchQA','English','Dataset from Jeapardy archives which consists of more than 140k question-answer pairs with each pair having 49.6 snippets on average.','140,000','JSON','Question Answering, Reading Comprehension','2017','Dunn et al.','https://github.com/nyu-dl/dl4ir-searchqA','https://arxiv.org/pdf/1704.05179.pdf',],
		['259','Semantic Parsing in Context (SParC)','English','Dataset consists of 4,298 coherent question sequences (12k+ unique individual questions annotated with SQL queries annotated byt. It is the context-dependent/multi-turn version of the Spider task.','4,298','JSON, SQL','Semantic Parsing, SQL-to-Text','2019','Yu et al.','https://github.com/taoyds/sparc','https://arxiv.org/pdf/1906.02285.pdf',],
		['260','Semantic Textual Similarity Benchmark','English','The task is to predict textual similarity between sentence pairs.','8,628','CSV','Semantic Similarity','2017','Cer et al.','http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark','https://www.aclweb.org/anthology/S17-2001.pdf',],
		['261','SemEval-2016 Task 4','English','Dataset contains 5 subtasks involving the sentiment analysis of tweets.','~75,000','Text','Classification, Sentiment Analysis','2016','Nakov et al.','http://alt.qcri.org/semeval2016/task4/index.php?id=data-and-tools','https://www.aclweb.org/anthology/S16-1001.pdf',],
		['262','SemEval-2019 Task 9 - Subtask A','English','Suggestion Mining from Online Reviews and Forums: Dataset contains corpora of unstructured text with the intent for mining it for suggestions.','~6,300','CSV','Suggestion Mining','2019','Negi et al.','https://github.com/Semeval2019Task9/Subtask-A','https://www.aclweb.org/anthology/S19-2151.pdf',],
		['263','SemEval-2019 Task 9 - Subtask B','English','Suggestion Mining from Hotel Reviews: Dataset contains corpora of unstructured text with the intent for mining it for suggestions.','~800','CSV','Suggestion Mining','2019','Negi et al.','https://github.com/Semeval2019Task9/Subtask-B','https://www.aclweb.org/anthology/S19-2151.pdf',],
		['264','SemEvalCQA','Arabic, English','Dataset for community question answering.','n/a','XML','Question Answering, Reading Comprehension','2016','Nakov et al.','http://alt.qcri.org/semeval2016/task3/index.php?id=data-and-tools','https://www.aclweb.org/anthology/S16-1083.pdf',],
		['265','Sentences Involving Compositional Knowledge (SICK)','English','Dataset contains sentence pairs, generated from two existing sets: the 8K ImageFlickr data set and the SemEval 2012 STS MSR-Video Description.','~10,000','Text','Semantic Similarity, Entailment','2014','Marelli et al.','https://zenodo.org/record/2787612','https://www.aclweb.org/anthology/S14-2001.pdf',],
		['266','Sentiment Corpus of App Reviews with Fine-grained Annotations in German (SCARE)','German','Dataset consists of fine-grained annotations for mobile application reviews from the Google Play Store. For each user review the mentioned application aspects, i.e., the design or the usability, as well as subjective phrases, which evaluate these aspects, are annotated. In addition, the polarity (positive, negative or neutral) of each subjective phrase is recorded as well as the relationship of an aspect to the main app in discussion. Requires emailing source for password to retrieve data.','800,000','CSV','Sentiment Analysis','2016','S?nger et al.','http://romanklinger.de/scare/','https://www.aclweb.org/anthology/L16-1178.pdf',],
		['267','Sentiment Labeled Sentences Dataset','English','Dataset contains 3000 sentiment labeled sentences.','3,000','Text','Classification, Sentiment Analysis','2015','Kotzias','https://archive.ics.uci.edu/ml/machine-learning-databases/00331','http://dkotzias.com/papers/GICF.pdf',],
		['268','Sentiment140','English','Sentiment140 is a dataset that can be used for sentiment analysis. A popular dataset, it is perfect to start off your NLP journey. Emotions have been pre-removed from the data. The final dataset has 6 features: polarity of the tweet, id of the tweet, date of the tweet, query, username of the tweeter, text of the tweet','1,578,627','CSV','Sentiment Analysis','2009','Go et al.','http://help.sentiment140.com/for-students','https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf',],
		['269','Shaping Answers with Rules through Conversation (ShARC)','English','ShARC is a Conversational Question Answering dataset focussing on question answering from texts containing rules.','32,000','JSON','Question Answering, Reading Comprehension','2018','Saeidi et al.','https://sharc-data.github.io/','https://www.aclweb.org/anthology/D18-1233.pdf',],
		['270','Short Answer Scoring','English','Student-written short-answer responses.','n/a','TSV','Scoring Classification','2012','The Hewlett Foundation','https://www.kaggle.com/c/asap-sas/overview','',],
		['271','Simplified Versions of the CommAI Navigation tasks (SCAN)','English','Dataset used for for studying compositional learning and zero-shot generalization. SCAN consists of a set of commands and their corresponding action sequences.','20,000+','Text','Compositional Learning','2018','Lake et al.','https://github.com/brendenlake/SCAN','https://arxiv.org/pdf/1711.00350.pdf',],
		['272','Situations With Adversarial Generations (SWAG)','English','Dataset consists of 113k multiple choice questions about grounded situations. Each question is a video caption from LSMDC or ActivityNet Captions, with four answer choices about what might happen next in the scene.','113,000','CSV','Question Answering, Reading Comprehension','2018','Zellers et al.','https://rowanzellers.com/swag/','https://arxiv.org/pdf/1808.05326.pdf',],
		['273','Skytrax User Reviews Dataset','English','User reviews of airlines, airports, seats, and lounges from Skytrax.','41,396','CSV','Classification, Sentiment Analysis','2015','Nguyen','https://github.com/quankiquanki/skytrax-reviews-dataset/tree/master/data','',],
		['274','SMS Spam Collection Dataset','English','Dataset contains SMS spam messages.','5,574','Text','Classification','2011','Almeida et al.','https://www.kaggle.com/uciml/sms-spam-collection-dataset','http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/',],
		['275','SNAP Social Circles: Twitter Database','English','Large Twitter network data.','Nodes: 81,306, Edges:1,768,149','Text','Clustering, Graph Analysis','2012','McAuley et al.','https://snap.stanford.edu/data/egonets-Twitter.html','http://i.stanford.edu/~julian/pdfs/nips2012.pdf',],
		['276','Soccer Dialogues','English','Dataset contains soccer dialogues over a knowledge graph','2,890','JSON','Knowledge Graphs, Dialogue','2019','SDA Lab, Uni. Of Bonn &amp; Volkswagen Research','https://github.com/SmartDataAnalytics/KG-Copy_Network/tree/master/soccer_conversations','',],
		['277','Social IQA','English','Dataset used fo question-answering benchmark for testing social commonsense intelligence.','37,000+','JSON','Question Answering, Commonsense','2019','Sap et al.','https://leaderboard.allenai.org/socialiqa/submissions/get-started','https://arxiv.org/pdf/1904.09728.pdf',],
		['278','Social Media Mining for Health (SMM4H)','English','Dataset contains medication-related text classification and concept normalization from Twitter','25,678','Text','Classification','2018','Sarker et al.','https://data.mendeley.com/datasets/rxwfb3tysd/2','https://www.aclweb.org/anthology/W18-5904.pdf',],
		['279','Social-IQ Dataset','English','Dataset containing videos and natural language questions for visual reasoning.','7,500','n/a','Question Answering, Visual, Commonsense','2019','Zadeh et al.','https://github.com/A2Zadeh/Social-IQ','http://openaccess.thecvf.com/content_CVPR_2019/papers/Zadeh_Social-IQ_A_Question_Answering_Benchmark_for_Artificial_Social_Intelligence_CVPR_2019_paper.pdf',],
		['280','Spambase Dataset','English','Dataset contains spam emails.','4,601','Text','Classification','1999','Hopkins et al.','https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/','',],
		['281','Spider 1.0','English','Dataset consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains.','10,181','JSON, SQL','Semantic Parsing, SQL-to-Text','2018','Yu et al.','https://github.com/taoyds/spider','https://arxiv.org/pdf/1809.08887.pdf',],
		['282','SQuAD v2.0','English','Paragraphs w/ questions and answers.','150,000','JSON','Question Answering, Reading Comprehension','2018','Rajpurkar et al.','https://rajpurkar.github.io/SQuAD-explorer/','https://arxiv.org/pdf/1806.03822.pdf',],
		['283','SQuAD-it','Italian','The dataset contains more than 60,000 question/answer pairs in Italian derived from the original English SQuAD dataset.','60,000+','JSON','Question Answering, Reading Comprehension','2018','Croce et al.','https://github.com/crux82/squad-it','http://ceur-ws.org/Vol-2481/paper25.pdf',],
		['284','Stack Exchange Data Explorer','English','An open source tool for running arbitrary queries against public data from the Stack Exchange network. Features include collaborative query editing for all graduated and public beta Stack Exchange sites. ','','','','','','https://data.stackexchange.com/','',],
		['285','Stack Overlow BigQuery Dataset','English','BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges.','n/a','n/a','Text Corpora','2018','Stack Overflow','https://www.kaggle.com/stackoverflow/stackoverflow','',],
		['286','Stanford','English','Stanford Question Answering Dataset (SQuAD) - Q&A','','','','','','https://rajpurkar.github.io/SQuAD-explorer/','',],
		['287','Stanford Natural Language Inference (SNLI) Corpus','English','Image captions matched with newly constructed sentences to form entailment, contradiction, or neutral pairs.','570,000','Text','Inference, Entailment','2015','Bowman et al.','https://nlp.stanford.edu/projects/snli/','https://nlp.stanford.edu/pubs/snli_paper.pdf',],
		['288','Switchboard Dialogue Act Corpus (SwDA)','English','A subset of the Switchboard-1 corpus consisting of 1,155 conversations and 42 tags','1,155','UTT','Dialogue Act Classification','1997','Bates et al.','http://compprag.christopherpotts.net/swda.html','https://web.stanford.edu/~jurafsky/ws97/asru97.pdf',],
		['289','TabFact','English','Dataset contains 16k Wikipedia tables as evidence for 118k human annotated statements to study fact verification with semi-structured evidence.','16,000','JSON','Natural Language Inference','2020','Chen et al.','https://github.com/wenhuchen/Table-Fact-Checking','https://arxiv.org/pdf/1909.02164.pdf',],
		['290','Taskmaster -2 ','English','Dataset consists of 17,289 dialogs in seven domains: restaurants (3276), food ordering (1050), movies (3047), hotels (2355), flights (2481), music (1602), and sports (3478). It consists entirely of spoken two-person dialogs.','17,289','JSON','Dialogue','2020','Byrne et al.','https://github.com/google-research-datasets/Taskmaster/tree/master/TM-2-2020','',],
		['291','Taskmaster-1','English','Dataset contains 13,215 task-based dialogs, including 5,507 spoken and 7,708 written dialogs created with two distinct procedures. Each conversation falls into one of six domains: ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations.','13,215','JSON','Dialogue','2019','Byrne et al.','https://research.google/tools/datasets/taskmaster-1/','https://arxiv.org/pdf/1909.05358.pdf',],
		['292','Ten Thousand German News Articles Dataset (10kGNAD)','German','Dataset consists of 10273 german language news articles from an austrian online newspaper categorized into nine topics.','10,273','CSV','Text Corpora','2019','Timo Block','https://github.com/tblock/10kGNAD','https://tblock.github.io/10kGNAD/',],
		['293','Tencent AI Lab Embedding Corpus','Chinese','Dataset provides 200-dimension vector representations, a.k.a. embeddings, for over 8 million Chinese words and phrases.','8M','Text','Embeddings','2018','Song et al.','https://ai.tencent.com/ailab/nlp/embedding.html','https://www.aclweb.org/anthology/N18-2028.pdf',],
		['294','Textbook Question Answering','English','The M3C task builds on the popular Visual Question Answering (VQA) and Machine Comprehension (MC) paradigms by framing question answering as a machine comprehension task, where the context needed to answer questions is provided and composed of both text and images.','26,620','JSON, PNG','Question Answering, Reading Comprehension, Visual','2017','Kembhavi et al.','http://data.allenai.org/tqa/','http://ai2-website.s3.amazonaws.com/publications/CVPR17_TQA.pdf',],
		['295','TextVQA','English','TextVQA requires models to read and reason about text in images to answer questions about them. Specifically, models need to incorporate a new modality of text present in the images and reason over it to answer TextVQA questions.','36,602','JSON, PNG','Question Answering, Visual, Commonsense','2019','Singh et al.','https://textvqa.org/dataset','https://arxiv.org/pdf/1904.08920.pdf',],
		['296','The Arabic Parallel Gender Corpus','Arabic','Dataset is designed to support research on gender bias in natural language processing applications working on Arabic. Requires to submit application for approval.','~12,000','n/a','Gender Identification','2019','Habash et al.','https://camel.abudhabi.nyu.edu/arabic-parallel-gender-corpus/','https://www.aclweb.org/anthology/W19-3822.pdf',],
		['297','The Benchmark of Linguistic Minimal Pairs (BLiMP)','English','BLiMP is a challenge set for evaluating what language models (LMs) know about major grammatical phenomena in English.','67 sub-datasets each with 1,000 minimal pairs','JSON','Language Modeling','2019','Warstadt et al.','https://github.com/alexwarstadt/blimp','https://arxiv.org/pdf/1912.00582.pdf',],
		['298','The Conversational Intelligence Challenge 2 (ConvAI2)','English','A chit-chat dataset based on PersonaChat dataset.','3,127','JSON','Dialogue','2018','NeurIPS','http://convai.io/data/','',],
		['299','The Corpus of Linguistic Acceptability','English','Dataset used to classifiy sentences as grammatical or not grammatical.','10,657','TSV','Grammatical Acceptability','2018','Warstadt et al.','https://nyu-mll.github.io/CoLA/','https://arxiv.org/pdf/1805.12471.pdf',],
		['300','The Cross-lingual Natural Language Inference corpus (XNLI)','Multi-Lingual','Dataset contains collection of 5,000 test and 2,500 dev pairs for the MultiNLI corpus. The pairs are annotated with textual entailment and translated into 14 languages: French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili and Urdu.','112,500','JSON, Text','Entailment','2018','Conneau et al.','http://www.nyu.edu/projects/bowman/xnli/','https://arxiv.org/pdf/1809.05053.pdf',],
		['301','The Dialog-based Language Learning Dataset','English','Dataset was designed to measure how well models can perform at learning as a student given a teacher?s textual responses to the student?s answer.','n/a','Text','Question Answering, Reading Comprehension','2016','Weston','https://research.fb.com/downloads/babi/','https://arxiv.org/pdf/1604.06045.pdf',],
		['302','The Emotion in Text','English','Dataset of tweets labelled with emotion. Categories: empty, sadness, enthusiasm, neutral, worry, sadness, love, fun, hate, happiness, relief, boredom, surprise, anger.','40,000','CSV','Emotion Classification','2016','CrowdFlower','https://github.com/sarnthil/unify-emotion-datasets/tree/master/datasets','https://www.researchgate.net/publication/318740457_Emotion_Intensities_in_Tweets',],
		['303','The Irish Times IRS','English','Dataset contains 23 years of events from Ireland.','1,425,460','CSV','Clustering, Events, Language Detection','2018','Kulkarni','https://www.kaggle.com/therohk/ireland-historical-news','',],
		['304','The Movie Dialog Dataset','English','Dataset measures how well models can perform at goal and non-goal orientated dialogue centered around the topic of movies (question answering, recommendation and discussion).','~3.5M','Text','Question Answering, Reading Comprehension','2016','Dodge et al.','https://research.fb.com/downloads/babi/','https://www.aclweb.org/anthology/I17-1099.pdf',],
		['305','The NewsReader MEANTIME Corpus','Multi-Lingual','480 news articles: 120 English Wikinews articles on four topics (i.e. Airbus and Boeing, Apple Inc., Stock market, and General Motors, Chrysler and Ford) and their translations in Spanish, Italian, and Dutch. Annotated with entities, events, temporal, semantic roles and event/entity coreference.','480','XML, NAF','Named Entity Recognition (NER)','2016','Minard et al.','http://www.newsreader-project.eu/results/data/wikinews/','http://www.lrec-conf.org/proceedings/lrec2016/pdf/488_Paper.pdf',],
		['306','The Penn Treebank Project','English','Naturally occurring text annotated for linguistic structure.','~1M words','Text','POS','1995','Marcus et al.','https://github.com/tomsercu/lstm/tree/master/data','http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&amp;rep=rep1&amp;type=pdf',],
		['307','The SimpleQuestions Dataset','English','Dataset for question answering with human generated questions paired with a corresponding fact, formatted as (subject, relationship, object), that provides the answer but also a complete explanation.','108,442','Text','Question Answering, Reading Comprehension','2015','Bordes et al.','https://research.fb.com/downloads/babi/','https://arxiv.org/pdf/1506.02075.pdf',],
		['308','The Stanford Sentiment Treebank (SST)','English','Sentence sentiment classification of movie reviews.','69,000','PTB','Sentiment Analysis','2013','Socher et al.','https://nlp.stanford.edu/sentiment/code.html','https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf',],
		['309','The Story Cloze Test | ROCStories','English','Dataset for story understanding that provides systems with four-sentence stories and two possible endings. The systems must then choose the correct ending to the story.','100,000+','JSON','Question Answering, Reading Comprehension','2017','Mostafazadeh et al.','https://cs.rochester.edu/nlp/rocstories/','https://www.cs.rochester.edu/~nasrinm/files/Papers/lsdsem17-shared-task.pdf',],
		['310','The TAC Relation Extraction Dataset (TACRED)','English','A relation extraction dataset containing 106k+ examples covering 42 TAC KBP relation types. Costs $25 for non-members.','106,264','CoNLL, JSON','Relation Extraction','2017','Yuhao et al.','https://nlp.stanford.edu/projects/tacred/','https://nlp.stanford.edu/pubs/zhang2017tacred.pdf',],
		['311','The WikiMovies Dataset','English','Dataset contains only the QA part of the Movie Dialog dataset, but using three different settings of knowledge: using a traditional knowledge base (KB), using Wikipedia as the source of knowledge, or using IE (information extraction) over Wikipedia.','~100,000','Text','Question Answering, Reading Comprehension','2016','Miller et al.','https://research.fb.com/downloads/babi/','https://arxiv.org/pdf/1606.03126.pdf',],
		['312','The Winograd Schema Challenge','English','Dataset to determine the correct referrent of the pronoun from among the provided choices.','150','XML','Coreference Resolution','2012','Levesque et al.','https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html','https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/4924',],
		['313','Topical-Chat','English','A knowledge-grounded human-human conversation dataset where the underlying knowledge spans 8 broad topics and conversation partners don?t have explicitly defined roles.','10,784','JSON','Dialogue','2019','Gopalakrishnan et al.','https://github.com/alexa/alexa-prize-topical-chat-dataset','https://m.media-amazon.com/images/G/01/amazon.jobs/3079_Paper._CB1565131710_.pdf',],
		['314','Total-Text-Dataset','English','Dataset used to classify curved text in pictures.','~1,500','JPG','Scene Text Detection','2019','Ch&#39;ng et al.','https://github.com/cs-chan/Total-Text-Dataset','https://arxiv.org/pdf/1710.10400.pdf',],
		['315','Train-O-Matic Large','Multi-Lingual','Automatically-generated corpora in multiple languages with sense annotations for nouns using WordNet for English and BabelNet for all other languages as inventories of senses.','10M+','XML','Word Sense Disambiguation ','2018','Pasini et al.','http://trainomatic.org/data/train-o-matic_lrec2018.tar.gz','https://www.aclweb.org/anthology/D17-1008.pdf',],
		['316','Train-O-Matic Small','Multi-Lingual','Automatically-generated corpora in multiple languages with sense annotations for nouns using WordNet for English and BabelNet for all other languages as inventories of senses.','1M+','XML','Word Sense Disambiguation ','2017','Pasini et al.','http://trainomatic.org/data/train-o-matic-data.tar.gz','https://www.aclweb.org/anthology/D17-1008.pdf',],
		['317','Translation-Augmented-LibriSpeech-Corpus (Libri-Trans)','English, French','Dataset is an augmentation of LibriSpeech ASR and contains English utterances (from audiobooks) automatically aligned with French text. It offers ~236h of speech aligned to translated text.','~236 Hours','Text, WAV','Speech Translation','2018',' Kocabiyikoglu et al.','https://github.com/alicank/Translation-Augmented-LibriSpeech-Corpus','https://www.aclweb.org/anthology/L18-1001.pdf',],
		['318','Trec CAR Dataset','English','Dataset contains topics, outlines, and paragraphs that are extracted from English Wikipedia (2016 XML dump). Wikipedia articles are split into the outline of sections and the contained paragraphs.','~285,000','CBOR','Information Retrieval','2019','Dietz et al.','http://trec-car.cs.unh.edu/datareleases/index.html','http://trec-car.cs.unh.edu/',],
		['319','TrecQA','English','Dataset is commonly used for evaluating answer selection in question answering.','n/a','XML','Question Answering, Reading Comprehension','2007','Wang et al.','https://github.com/castorini/data/tree/master/TrecQA/data','https://www.aclweb.org/anthology/D07-1003.pdf',],
		['320','T-REx','English','Dataset contains Wikipedia abstracts aligned with Wikidata entities.','11M aligned triples','JSON and NIF','Relation Extraction','2018','Elsahar et al.','https://hadyelsahar.github.io/t-rex/downloads/','https://www.aclweb.org/anthology/L18-1544.pdf',],
		['321','TriviaQA','English','Dataset containing over 650K question-answer-evidence triples. It includes 95K QA pairs authored by trivia enthusiasts and independently gathered evidence documents, 6 per question on average.','650,000+','JSON','Question Answering, Reading Comprehension','2017','Joshi et al.','http://nlp.cs.washington.edu/triviaqa/','https://arxiv.org/pdf/1705.03551.pdf',],
		['322','TupleInf Open IE Dataset','English','Dataset contains Open IE tuples extracted from 263K sentences that were used by the solver in &#34;Answering Complex Questions Using Open Information Extraction&#34; (referred as Tuple KB, T).','263,000','Text','Knowledge Base','2017','Allen Institute','http://data.allenai.org/tuple-ie/','https://arxiv.org/pdf/1704.05572.pdf',],
		['323','Twenty Newsgroups Dataset','English','Messages from 20 different newsgroups.','20,000','Text','Classification, Clustering','1999','Mitchell et al.','http://qwone.com/~jason/20Newsgroups/','http://www.kamalnigam.com/papers/emcat-mlj99.pdf',],
		['324','Twitter','English','Twitter -  41.7 million user profiles, 1.47 billion social relations, 4,262 trending topics, and 106 million tweets.','','Text','Classification, Clustering','2010','','http://an.kaist.ac.kr/traces/WWW2010.html','',],
		['325','Twitter Chat Corpus','English','Dataset contains Twitter question-answer pairs.','5M','Text','Dialogue','2017','Marsan Ma','https://github.com/marsan-ma/chat_corpus','https://github.com/Marsan-Ma/chat_corpus',],
		['326','Twitter Dataset for Arabic Sentiment Analysis','Arabic','Dataset contains Arabic tweets.','2,000','Text','Classification, Sentiment Analysis','2014','Abdulla','https://archive.ics.uci.edu/ml/machine-learning-databases/00293/','https://archive.ics.uci.edu/ml/datasets/Twitter%2BData%2Bset%2Bfor%2BArabic%2BSentiment%2BAnalysis#',],
		['327','Twitter US Airline Sentiment','English','Contributors were asked to classify positive, negative, and neutral tweets, followed by categorizing negative reasons.','14,500','CSV','Classification, Sentiment Analysis','2016','Figure Eight','https://www.kaggle.com/crowdflower/twitter-airline-sentiment','',],
		['328','Twitter100k','English','Pairs of images and tweets.','100,000','Text and Images','Multi-Modal Learning','2017','Hu et al.','https://github.com/huyt16/Twitter100k','https://arxiv.org/pdf/1703.06618.pdf',],
		['329','TyDi QA','Multi-Lingual','TyDi QA includes question-answer pairs from 11 languages: Arabic, Bengali, English, Finnish, Indonesian, Kiswahili, Russian. Japanese, Korean, Thai, and Telugu.','204,000','JSON','Question Answering, Reading Comprehension','2020','Clark et al.','https://github.com/google-research-datasets/tydiqa','https://ai.googleblog.com/2020/02/tydi-qa-multilingual-question-answering.html',],
		['330','Ubuntu Dialogue Corpus','English','Dialogues extracted from Ubuntu chat stream on IRC.','930,000 ','CSV','Text Corpora, Dialogue','2015','Lowe et al.','https://www.kaggle.com/rtatman/ubuntu-dialogue-corpus','https://www.aclweb.org/anthology/W15-4640.pdf',],
		['331','United Nations Parallel Corpus','Multi-Lingual','Parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages: Arabic, Chinese, English, French, Russian, and Spanish.','799,276','TEI, XML','Machine Translation','2016','Ziemski et al.','https://conferences.unite.un.org/UNCORPUS/','https://www.aclweb.org/anthology/L16-1561.pdf',],
		['332','Urban Dictionary Dataset','English','Corpus of words, votes and definitions.','2,606,522','CSV','Reading Comprehension','2016-05','Anonymous','https://www.kaggle.com/therohk/urban-dictionary-words-dataset','https://www.kaggle.com/therohk/urban-dictionary-words-dataset',],
		['333','UseNet Corpus','English','UseNet forum postings.','7B','Text','Dialogue','2011','Shaoul et al.','https://www.psych.ualberta.ca/~westburylab/downloads/usenetcorpus.download.html','https://www.psych.ualberta.ca/~westburylab/downloads/usenetcorpus.download.html',],
		['334','Video Commonsense Reasoning (VCR)','English','Dataset contains 290K multiple-choice questions on 110K images.','290,000','JSON, JPG','Question Answering, Visual, Commonsense','2018','Zellers et al.','https://visualcommonsense.com/download/','https://arxiv.org/pdf/1811.10830.pdf',],
		['335','VisDial','English','Dataset contains images from COCO training set, and dialogues. Meant to be used for model to be trained in answering questions about images during conversation. Contains 1.2M dialog question-answers.','1.2M','JSON','Question Answering, Visual, Dialogue','2017','Das et al.','https://visualdialog.org/data','https://arxiv.org/pdf/1703.06585.pdf',],
		['336','Visual QA (VQA)','English','Dataset containing open-ended questions about images. These questions require an understanding of vision, language and commonsense to answer.','265,016 images','JSON','Visual Question Answering','2015','Antol et al.','https://visualqa.org/download.html','https://arxiv.org/pdf/1505.00468.pdf',],
		['337','Voices Obscured in Complex Environmental Settings (VOiCES)','English','Dataset contains a total of 15 hours (3,903 audio files) in male and female read speech.','n/a','Wav','Speech Recognition','2018','Various','https://voices18.github.io/downloads/','https://arxiv.org/pdf/1804.05053.pdf',],
		['338','VoxCeleb','Multi-Lingual','An audio-visual dataset consisting of short clips of human speech, extracted from interview videos uploaded to YouTube.','n/a','MD5, URL','Speech Recognition, Visual','2017','Nagrani et al.','http://www.robots.ox.ac.uk/~vgg/data/voxceleb/','https://arxiv.org/pdf/1706.08612.pdf',],
		['339','WAT 2019 Hindi-English','Hindi, English','Dataset consists of multimodal English-to-Hindi translation. It inputs an image, rectangular region in the image and english caption. It outputs a caption in Hindi.','32,925','Text, JPEG','Machine Translation, Multi-Modal Learning','2019','Parida et al.','https://ufal.mff.cuni.cz/hindi-visual-genome/wat-2019-multimodal-task','https://www.aclweb.org/anthology/D19-5224.pdf',],
		['340','Watan-2004 Corpus','Arabic','Dataset contains about 20,000 articles talking about 6 topics: culture, religion, economy, local news, international news and sports.','20,000','HTML','Text Corpora','2004','Abbas et al.','https://sites.google.com/site/mouradabbas9/corpora','',],
		['341','Web Inventory of Transcribed and Translated Talks (WIT3)','Multi-Lingual','Dataset contains a collection of transcribed and translated talks. The core of the dataset is from Ted Talks corpus. As of 2016, It holds 109 languages.','n/a','XML','Machine Translation','2012','Cettolo et al.','https://wit3.fbk.eu/mono.php?release=XML_releases&amp;tinfo=cleanedhtml_ted','https://pdfs.semanticscholar.org/c64d/27b122d5b6ef0be135e63df05c3b24bd80c5.pdf?_ga=2.127716493.1668670113.1583626265-958501894.1582324561',],
		['342','Web of Science Dataset','English','Hierarchical Datasets for Text Classification.','46,985','Text','Classification','2017','Kowsari et al.','https://data.mendeley.com/datasets/9rw3vkcfy4/6','https://arxiv.org/pdf/1709.08267.pdf',],
		['343','Webhose','English','Webhose?s free datasets include data from a range of different sources, languages and categories.','','','','','','https://webhose.io/free-datasets/','',],
		['344','Webis-CLS-10','Multi-Lingual','The Cross-Lingual Sentiment (CLS) dataset comprises about 800,000 Amazon product reviews in the 4 languages: English, German, French, and Japanese.','800,000','Tar','Classification, Sentiment Analysis','2010','Prettenhofer et al.','https://webis.de/data/webis-cls-10.html','https://webis.de/downloads/publications/papers/stein_2010k.pdf',],
		['345','Webis-Snippet-20 Corpus','English','Dataset comprises four abstractive snippet dataset from ClueWeb09, Clueweb12, and DMOZ descriptions. More than 10 million &lt;webpage, abstractive snippet&gt; pairs / 3.5 million &lt;query, webpage, abstractive snippet&gt; pairs were collected.','3.5M','JSON','Summarization','2020','Chen et al.','https://zenodo.org/record/3653834#.XoD9aIhKiUk','https://arxiv.org/pdf/2002.10782.pdf',],
		['346','Webis-TLDR-17 Corpus','English','Dataset contains 3 Million pairs of content and self-written summaries mined from Reddit. It is one of the first large-scale summarization dataset from the social media domain.','3,084,410','JSON','Summarization','2017','Volske et al.','https://zenodo.org/record/1168855#.XoD8i4hKiUk','https://www.aclweb.org/anthology/W17-4508.pdf',],
		['347','WebQuestions Semantic Parses Dataset','English','Dataset contains full semantic parses in SPARQL queries for 4,737 questions, and ?partial? annotations for the remaining 1,073 questions for which a valid parse could not be formulated or where the question itself is bad or needs a descriptive answer.','5,810','JSON','Semantic Parsing','2016','Yih et al.','https://www.microsoft.com/en-us/download/details.aspx?id=52763','https://www.aclweb.org/anthology/P16-2033.pdf',],
		['348','Who Did What Dataset','English','Dataset contains over 200,000 fill-in-the-gap (cloze) multiple choice reading comprehension problems constructed from the LDC English Gigaword newswire corpus.','200,000K','XML','Question Answering, Reading Comprehension','2016','Onishi et al.','https://tticnlp.github.io/who_did_what/download.html','https://arxiv.org/pdf/1608.05457.pdf',],
		['349','WikiAnn','Multi-Lingual','Dataset with NER annotations for PER, ORG and LOC. It has been constructed using the linked entities in Wikipedia pages for 282 different languages.','95,924','JSON','Named Entity Recognition (NER)','2017','Pan et al.','https://elisa-ie.github.io/wikiann/','https://www.aclweb.org/anthology/P17-1178.pdf',],
		['350','Wikidata NE dataset','English, German','Dataset has 2 parts: the Named Entity files and the link files. The Named Entity files include the most important information about the entities, whereas the link files contain the links and ids in other databases.','n/a','JSON','Named Entity Recognition, Knowledge Base','2017','Gei? et al.','https://event.ifi.uni-heidelberg.de/?page_id=532','https://www.researchgate.net/publication/322281320_NECKAr_A_Named_Entity_Classifier_for_Wikidata',],
		['351','WikiHow','English','Dataset contains article and summary pairs extracted and constructed from an online knowledge base written by different human authors.','230,000+','Text','Text Corpora, Summarization','2018','Koupaee et al.','https://github.com/mahnazkoupaee/WikiHow-Dataset','https://arxiv.org/pdf/1810.09305.pdf',],
		['352','WikiLinks','English','Dataset contains 40 million mentions over 3 million entities based on hyperlinks from Wikipedia.','~10M','Text','Text Corpora','2012','Singh et al.','http://www.iesl.cs.umass.edu/data/data-wiki-links','http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=25C910ACB8DEEFFD224AE2FF7B9C86AE?doi=10.1.1.385.826&amp;rep=rep1&amp;type=pdf',],
		['353','WikiMatrix','Multi-Lingual','Dataset contains 135 million parallel sentences for 1,620 different language pairs in 85 different languages.','135M','TSV','Machine Translation','2019','Schwenk et al.','https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix','https://arxiv.org/pdf/1907.05791.pdf',],
		['354','Wikimedia','English','Wikimedia data dumps','','','','','','https://dumps.wikimedia.org/','',],
		['355','Wikipedia','English','The 2016-12-21 dump of English Wikipedia.','5,075,182','SQL','Text Corpora','2016','Facebook Research','https://dl.fbaipublicfiles.com/drqa/docs.db.gz, https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia, https://nlp.cs.nyu.edu/wikipedia-data/','',],
		['356','Wikipedia Links','English','The full text of Wikipedia. The dataset contains almost 1.9 billion words from more than 4 million articles. You can search by word, phrase or part of a paragraph itself.','','','','','','https://code.google.com/archive/p/wiki-links/downloads','',],
		['357','Wikipedia News Corpus','English','Text from Wikipedia&#39;s current events page with dates.','~25,000','Text','Text Corpora','2019','Parth Parikh','https://gist.github.com/pncnmnp/3321df2e82eb9b8b1b2f59131c7144b2','',],
		['358','WikiQA Corpus','English','Dataset contains Bing query logs as the question source. Each question is linked to a Wikipedia page that potentially has the answer. ','3,047','TSV','Question Answering, Reading Comprehension','2015','Yang et al.','https://www.microsoft.com/en-us/download/details.aspx?id=52419','https://www.aclweb.org/anthology/D15-1237.pdf',],
		['359','WikiReading','Multi-Lingual','The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. Includes English, Russian and Turkish.','18M','JSON','Knowledge Base, NLU','2016','Hewlett &amp; Kenter et al.','https://github.com/google-research-datasets/wiki-reading','https://arxiv.org/pdf/1608.03542.pdf',],
		['360','WikiSplit','English','Dataset contains 1 million English sentences, each split into two sentences that together preserve the original meaning, extracted from Wikipedia edits.','1M','TSV','Sentence Simplification','2018','Botha et al.','https://github.com/google-research-datasets/wiki-split','https://arxiv.org/pdf/1808.09468.pdf',],
		['361','WikiSQL','English','A large collection of automatically generated questions about individual tables from Wikipedia.','80,654','JSON','Semantic Parsing, Text-to-SQL','2017','Zhong et al.','https://github.com/jkkummerfeld/text2sql-data','https://arxiv.org/pdf/1709.00103.pdf',],
		['362','WikiText-103 &amp; 2','English','Dataset contains word and character level tokens extracted from Wikipedia','100M+','TOKENS','Language Modeling','2016','Merity et al.','https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/','https://arxiv.org/pdf/1609.07843.pdf',],
		['363','Winogender Schemas','English','Dataset with pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias in automated coreference resolution systems.','720','TSV','Coreference Resolution','2018','Rudinger et al.','https://github.com/rudinger/winogender-schemas','https://arxiv.org/pdf/1804.09301.pdf',],
		['364','WinoGrande','English','Formulated as a fill-in-a-blank task with binary options, the goal is to choose the right option for a given sentence which requires commonsense reasoning.','44,000','JSON','Commonsense Reasoning','2019','Sakaguchi et al.','https://leaderboard.allenai.org/winogrande/submissions/get-started','https://arxiv.org/pdf/1907.10641.pdf',],
		['365','Wisesight Sentiment Corpus','Thai','Dataset contains around 26,700 messages in Thai language from various social media with human-annotated sentiment classification (positive, neutral, negative, and question).','~26,700','Text','Classification, Sentiment Analysis','2019','Wisesight','https://github.com/PyThaiNLP/wisesight-sentiment','',],
		['366','WMT 14 English-German','Multi-Lingual','Sentence pairs for translation.','4.5M','Text','Machine Translation','2015','Stanford','https://nlp.stanford.edu/projects/nmt/','https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf',],
		['367','WMT 15 English-Czech','Multi-Lingual','Sentence pairs for translation.','15.8M','Text','Machine Translation','2016','Stanford','https://nlp.stanford.edu/projects/nmt/','https://nlp.stanford.edu/pubs/luong2016acl_hybrid.pdf',],
		['368','WMT 19 Multiple Datasets','Multi-Lingual','Multiple text corpora in multiple languages.','n/a','Text','Text Corpora, Machine Translation','2019','ACL Workshop','http://www.statmt.org/wmt19/translation-task.html','',],
		['369','WordNet','English','WordNet is a large database of English synsets. Synsets are groups of synonyms that each describe a different concept. WordNet?s structure makes it a very useful tool for NLP.  ','','','','','','https://wordnet.princeton.edu/','',],
		['370','Words in Context','English','Dataset for evaluating contextualized word representations.','2,400','Text','Word Sense Disambiguation','2019','Pilehvar et al.','https://pilehvar.github.io/wic/','https://arxiv.org/pdf/1808.09121.pdf',],
		['371','Worldwide News - Aggregate of 20K Feeds','Multi-Lingual','One week snapshot of all online headlines in 20+ languages.','1,398,431','CSV','Clustering, Events, Machine Translation','2017','Kulkarni','https://www.kaggle.com/therohk/global-news-week','https://pdfs.semanticscholar.org/48cd/042d645e60b125fd5b90f734900dae4211d8.pdf',],
		['372','WSD English All-Words Fine-Grained Datasets','English','Unified five standard all-words Word Sense Disambiguation datasets.','7,000+','XML','Word Sense Disambiguation ','2017','Raganato et al.','http://lcl.uniroma1.it/wsdeval/data/WSD_Evaluation_Framework.zip','',],
		['373','XQuAD','Multi-Lingual','Dataset consists of a subset of 240 context paragraphs and 1,190 question-answer pairs from the development set of SQuAD v1.1  with their translations in 10 languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi.','1,190 ','JSON','Question Answering, Reading Comprehension','2019','Artetxe et al.','https://github.com/deepmind/xquad','https://arxiv.org/pdf/1910.11856.pdf',],
		['374','X-Stance','Multi-Lingual','Dataset contains more than 150 political questions, and 67k comments written by candidates on those questions. The questions are available in German,  French, Italian and English.','67,000','JSON','Stance Detection','2020','Vamvas et al.','https://github.com/ZurichNLP/xstance','https://arxiv.org/pdf/2003.08385.pdf',],
		['375','X-Sum','English','The XSum dataset consists of 226,711 Wayback archived BBC articles (2010 to 2017) and covering a wide variety of domains: News, Politics, Sports, Weather, Business, Technology, Science, Health, Family, Education, Entertainment and Arts.','226,711','JSON','Summarization','2018','Narayan et al.','https://github.com/EdinburghNLP/XSum','https://arxiv.org/pdf/1808.08745.pdf',],
		['376','Yahoo Webscope Program','English','Yahoo Webscope Program is a reference library of interesting and scientifically useful datasets for non-commercial use by academics and other scientists. ','','','','','','https://webscope.sandbox.yahoo.com/#datasets','',],
		['377','Yahoo! Music User Ratings of Musical Artists','English','Over 10M ratings of artists by Yahoo users. May be used to validate recommender systems or collaborative filtering algorithms.','~10M','Text','Clustering, PCA','2004','Yahoo!','https://webscope.sandbox.yahoo.com/catalog.php?datatype=r','https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/ILAT5B',],
		['378','Yelp Open Dataset','English','Dataset containing millions of reviews on Yelp. In addition it contains business data including location data, attributes, and categories.','6,685,900','JSON','Classification, Sentiment Analysis','2015','Yelp','https://www.yelp.com/dataset','https://www.yelp.com/dataset',],
		['379','YouTube Comedy Slam Preference Dataset','English','User vote data for pairs of videos shown on YouTube. Users voted on funnier videos.','1,138,562','Text','Classification','2012','Google','https://archive.ics.uci.edu/ml/machine-learning-databases/00223/','https://ai.googleblog.com/2012/02/quantifying-comedy-on-youtube-why.html',],
		/* auto-generated - END */
	];



$(document).ready(function() {	$('#dynamic').html( '<table cellpadding="0" style="width:100%" cellspacing="0" border="0" class="display" id="example"></table>' );
	$('#example').dataTable( {
		"aaData": aDataSet,
		"aoColumns": [
			{ "sTitle": "S/N", "sWidth": "5%" },
			{ "sTitle": "Name", "sWidth": "25%" },
			{ "sTitle": "Language", "sWidth": "5%" },
			{ "sTitle": "Description", "sWidth": "30%" },
			{ "sTitle": "Records", "sWidth": "5%" },
			{ "sTitle": "Format", "sWidth": "5%" },
			{ "sTitle": "Category", "sWidth": "5%" },
			{ "sTitle": "Year", "sWidth": "5%" },
			{ "sTitle": "Created By", "sWidth": "5%" },
			{ "sTitle": "URL", "sWidth": "5%" },
			{ "sTitle": "Paper", "sWidth": "5%" },
		]
		} );
	} );